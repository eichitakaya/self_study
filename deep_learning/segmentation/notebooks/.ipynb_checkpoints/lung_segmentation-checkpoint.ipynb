{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13147ac-d640-4369-b53e-dfe23c83b63f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **肺野セグメンテーションをやってみよう**\n",
    "画像処理における「セグメンテーション」とは，ある画像における関心領域を色塗りしたり，輪郭を特定したりする作業のことを言います． \\\n",
    "今回は，肺野セグメンテーションと称して，胸部X線単純写真から肺野を抽出する課題に挑戦してみましょう．\\\n",
    "\\\n",
    "データセットとしては，[日本放射線技術学会 画像部会 miniJSRT_database](http://imgcom.jsrt.or.jp/minijsrtdb/)が公開してくださっているSegmentation01を利用します(図１)．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78716cb-c39c-4803-9fef-7b3273172d36",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1VhdvY9JA8hB9R2T55KCqec2Gi7H-CR0n\" width=\"70%\"> \n",
    "<br>\n",
    "図1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a8e52-7557-46ad-9e36-ac1c9fe04be6",
   "metadata": {},
   "source": [
    "## **深層学習によるセグメンテーション**\n",
    "セグメンテーションの課題に対しては，「ピクセル毎の分類問題を解く」という問題設定で深層学習を適用することが一般的です．\n",
    "最も簡単な例としては，正方形のパッチを切り取っては，中心のピクセルが関心領域であるかを分類することを，ピクセル数だけ繰り返す方法があります（図2）．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8366fb-4794-4536-b15e-3a38ee7c21b6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1TsLF3fkFbdpP5X9RfKHC1S5DFbI_MAXV\" width=\"70%\"> \n",
    "<br>\n",
    "図2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed829e0a-a617-40f7-9e7f-9e422d448ab9",
   "metadata": {},
   "source": [
    "これにより，実質的にはピクセルの数だけ画像がある問題としてみなすことができ，比較的少ない症例数でも所望の結果を出しやすくなります．\\\n",
    "しかし，この手法では，あまり広い範囲を考慮して分類することができない点や，ピクセルの数だけ行われる学習や推論を効率的に行うことが難しい点が課題になります．これらを解決するアイデアとして，Fully Convolutional Networks（FCN）[引用]という深層学習モデルが開発されました（図3）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82932ace-a283-4894-b384-7ac422aeeb4b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1XUW-uNNhPtDHd26AEcm1LLXkS8hHZ56v\" width=\"70%\"> \n",
    "<br>\n",
    "図3　Fully Convolutional Networks[引用]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00a61e-d51c-4ed5-b0d6-c95ecf0d20ce",
   "metadata": {},
   "source": [
    "そして，このFCNを改良し，現時点で医用画像セグメンテーションにおけるデファクトスタンダードとして利用されるようになったのがU−Netです【引用】（図4）．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84485497-a902-40c8-9846-8937ba09814f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1H-VmyDCTV1IU9-o8AihHsJ6wHCJWJjn-\" width=\"70%\"> \n",
    "<br>\n",
    "図4　U-Net[引用]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed817e20-bcae-4c0c-bfdb-0cdc28ed0690",
   "metadata": {},
   "source": [
    "\n",
    "今回取り組む肺野セグメンテーションでは，このU-Netを使っていきましょう！\\\n",
    "全体の流れを以下に示します．\n",
    "\n",
    "## **プログラム全体の流れ**\n",
    "- データローダの作成\n",
    "- U-Netの定義\n",
    "- 誤差関数の定義\n",
    "- 学習ループの設計\n",
    "- モデルの学習\n",
    "- モデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e6206-3c79-434e-8e73-cdc02e17f5bc",
   "metadata": {},
   "source": [
    "### **データローダの作成**\n",
    "深層学習を実践する上で最も重要かつ面倒なのは，画像等のデータを読み込む部分（データローダ）を作り込むことだと言っても過言ではありません．作り方の方針は色々考えられますが，ここではPytorchを使った実践でよく行われる方法を説明します．\\\n",
    "データローダの実装にあたり，まずはミニバッチ学習という概念を思い出しましょう（図5）．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec0884-7ed4-4c00-9727-4880f4275d59",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1rWgWkMT0LLv17p21C2NQMjPh_bkJ_Fcu\" width=\"70%\"> \n",
    "<br>\n",
    "図5　ミニバッチ学習のおさらい\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918c867-488f-45b7-ad3e-0cf154bd4829",
   "metadata": {},
   "source": [
    "深層学習では，手持ちのデータを1つずつ入力したり，すべてまとめて入力したりするのではなく，ミニバッチというグループ単位で入力し，パラメータの調整が行われることが一般的です．\\\n",
    "この処理を実現するためには，手持ちのデータをグループに分け，それらのグループを順番に取り出すような機能を持つデータローダを作る必要があります．なおかつ，データの読み込みが一巡したら全体をシャッフルし，改めてグループ分けを行う機能も必要です．さらには，データをニューラルネットワークにすぐに流し込めるように，予め最適な形に変換しておく機能も欲しいところです．\\\n",
    "要求がたくさんあって混乱してくるかもしれませんが，Pytorchでは，\n",
    "- どのデータを使うか\n",
    "- そのデータはいくつあるのか\n",
    "- それぞれのデータはどのような形であってほしいのか\n",
    "\n",
    "以上3つをきちんと記述すれば，上記の要求をすべて満たすことができてしまいます．\n",
    "\n",
    "では実際にデータローダを設計していきましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357ae12-59b1-481b-9d74-9a29ee92780e",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "配布したデータのフォルダ構造は，以下のようになっています．\\\n",
    "data\\\n",
    "&emsp; \\- train\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\\\n",
    "&emsp; \\- val\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\\\n",
    "&emsp; \\- test\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\n",
    "\n",
    "データをtrain, validation, testに分ける意義については，【リンク】をご覧ください．\\\n",
    "画像部会が公開している形式では，trainとtestのみに分かれていますが，配布資料一式の中には，あらかじめtrainのうち10例をvalとして分け直したものが含まれています．\n",
    "\n",
    "上記のフォルダ構造を念頭に置きながら，データローダを作ります．\\\n",
    "まずは必要なライブラリをインポートしましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f51d2-8743-49ac-bfac-c83ebc558bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7a8b3-08ff-4836-ac9d-a22035482877",
   "metadata": {},
   "source": [
    "numpyは，ベクトルや行列の演算を行うためのライブラリです．使用頻度が高く，機能を呼び出す度にいちいちnumpyと書くのは面倒なので，プログラム上ではnpと略して使うことが一般的です（「as \\~」 は 「\\~と略して使いますよ」という宣言です）．\\\n",
    "\\\n",
    "torchは，言わずとしれた深層学習ライブラリ Pytorch です．\\\n",
    "from torch\\~ は必ずしも必要はありませんが，Pytorch の機能に含まれているDatasetとDataLoaderを使う際に毎回 torch.utils.data.Dataset などと宣言する必要があり冗長となってしまいます．上記のように書くことで，torch.utils.data を省略して Dataset を使うことができます．\\\n",
    "\\\n",
    "PILは，Pillowと呼ばれる画像処理ライブラリです．そこに含まれる Image という機能のみをインポートします．これは画像の読み込みや保存などを行うために使用します．\\\n",
    "\\\n",
    "globは，フォルダの中に含まれるファイル名を取り出す際に用いるライブラリです．\\\n",
    "\\\n",
    "この後にも，必要に応じてPythonのライブラリをインポートすることがあります．\n",
    "\n",
    "先述した3つの要素\n",
    "- どのデータを使うか\n",
    "- そのデータはいくつあるのか\n",
    "- それぞれのデータはどのような形であってほしいのか\n",
    "\n",
    "\n",
    "については，ChestDatasetという自作のクラスの中で定義していきます．これは，便利なデータローダを実現するための設計図のようなものだと理解しておきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b75cf-f0f7-461d-be23-dad17772d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, TrainValTest=\"train\"):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        どのデータを使うのかを記述する部分\n",
    "        \"\"\"\n",
    "        if TrainValTest == \"train\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/train/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/train/label/*\"))\n",
    "        elif TrainValTest == \"val\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/val/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/val/label/*\"))\n",
    "        elif TrainValTest == \"test\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/test/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/test/label/*\"))    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        データがいくつあるのかを数える\n",
    "        \"\"\"\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        データをどのような形で取り出すのか記述する\n",
    "        \"\"\"\n",
    "        image_path = self.img_path_list[index] # ファイル名\n",
    "        label_path = self.label_path_list[index] # ファイル名\n",
    "        \n",
    "        img = Image.open(image_path) # ファイル名を与えて画像を取り出す\n",
    "        img = np.array(img) # 画像をnumpy形式の行列へ変換\n",
    "        img = np.expand_dims(img, 0) # 1チャンネルであることを明示する（256, 256）→ (1, 256, 256)\n",
    "        img = torch.tensor(img) # 行列をpytorchで扱える形式（tensor型）に変換する\n",
    "        img = img / 255 # 0~255までの値を0~1までの値に変換する\n",
    "        \n",
    "        label = Image.open(label_path) # ファイル名を与えてラベルを取り出す\n",
    "        label = np.array(label) # ラベルをnumpy形式の行列へ変換\n",
    "        label = np.expand_dims(label, 0)\n",
    "        label = torch.tensor(label)\n",
    "        label = label = label / 255 # 肺野領域は255，それ以外は0となっているので，255で割って0 or 1に変換する\n",
    "        label = label.float() # 行列の値をfloat型に変換する（pytorchの都合）\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed28bb-1b95-4cf7-8c7f-edae3a75b7f9",
   "metadata": {},
   "source": [
    "「どのデータを使うか」については，\\__init__()の部分に記述しています．glob関数を用いて，フォルダから画像ファイル名のリストを取得しています．\\\n",
    "ユーザーの指定に従って利用するデータを分けられるように，train,val,testで場合分けしてあります．\n",
    "\n",
    "\\__len__()の部分では，上で読み込まれた画像ファイルリストが何枚あるのかを確認するために，len()を用いてリストの長さを出力するようにしています．すなわち，「そのデータはいくつあるのか」を確かめる機能ができあがりました．\n",
    "\n",
    "やや長くてめんどくさそうなのが__getitem__()の中身です．ここでは，ファイル名を用いた画像読み込みからスタートして，最後にreturnされるimg, labelが理想的な形になるようにデータの変換を行います．「それぞれのデータがどのような形になっていてほしいのか」を意識しながら変換を繰り返していきましょう．\\\n",
    "各行でどのような変換が行われているのかは，各行の#の後に付記してあります．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347b3a6-3d9d-492d-8633-072f575afcfa",
   "metadata": {},
   "source": [
    "さて，ここまででChestDatasetという設計図が完成しましたが，実際にこれを使うには「実体化（プログラミングの専門用語ではインスタンス化と呼ばれます）」をする必要があります．\\\n",
    "実体化は次のように行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b8d20-ecd9-416e-bbf2-5fd63bb5d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_train = ChestDataset(TrainValTest=\"train\")\n",
    "chest_val = ChestDataset(TrainValTest=\"val\")\n",
    "chest_test = ChestDataset(TrainValTest=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ff9ce-2717-4829-9b93-12e17b85cb85",
   "metadata": {},
   "source": [
    "1行目では，ChestDatasetをchest_trainという名前で実体化しました．TrainValTest=\"train\"という記述がありますが，これはChestDatasetの\\__init__()でどのフォルダから画像ファイルパスを取り出すのかを指定しています．\\\n",
    "2~3行目でも，同様にchest_valとchest_testを実体化しています．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3763de4-eb9f-4320-94f5-a67ef62cc54a",
   "metadata": {},
   "source": [
    "ここまでくれば，あとは簡単です．\\\n",
    "「実体化されたデータセットをDataLoaderで包み込む」という作業を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9a6c5-e8eb-4ebf-bdab-128c66af810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(chest_train, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(chest_val, batch_size=5, shuffle=False)\n",
    "test_loader = DataLoader(chest_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558dbc6-f73e-47ba-9135-352457427ba7",
   "metadata": {},
   "source": [
    "PytorchのDataLoaderには，あるデータセットに対して，グループ化を行って順番に取り出す機能や，中身をシャッフルする機能が備わっています．\n",
    "ここでchest_trainなどを包み込む際にbatch_size=5, shuffle=Trueという記述があるのは，ミニバッチに含まれる要素数とシャッフル機能を使うかどうかを指定するためです．\\\n",
    "このように指定するだけで，train_loaderは，データを5つずつ順番に取り出し，なおかつ取り出しが一巡したらシャッフルする機能を持ったことになります．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714eed25-42f9-4339-8da9-4c87141a0bbf",
   "metadata": {},
   "source": [
    "train_loaderがきちんと画像を読み込めるか確認しましょう．\\\n",
    "ここで作成したデータローダは，for文のenumerateという機能を使って発動することができます．\\\n",
    "実際に，for文の中でiやdataをprintして遊んでみましょう．\\\n",
    "また，dataのshapeについても同様にprintしてみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87fb66-f515-4737-9f27-7137e89417f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa51cd9-0669-437d-a509-7a784e725398",
   "metadata": {},
   "source": [
    "shapeをprintしたときに表示される[5, 1, 256, 256]は，それぞれ[バッチサイズ，チャンネル数，画像の幅，画像の高さ]を意味しています．\\\n",
    "Pytorchではこの形でデータをやりとりする必要があるので，そのために__getitem__()でいろいろな変換を施していました．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c63366-3694-4386-8e7a-f376fa346cdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **ネットワークの定義**\n",
    "次に，深層学習のメイン要素である，ニューラルネットワークを構築していきましょう．\\\n",
    "先述したように，ここではセグメンテーションタスクでよく利用されるU-Net[引用]と呼ばれるネットワークを実装します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572f467-2d3d-4297-802f-3b3b4dbae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=2, input_channel=1, output_channel=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.enco1_1 = nn.Conv2d(self.input_channel, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.enco2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco5_1 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco5_2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco6_1 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco6_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco7_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco7_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco8_1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco8_2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco9_1 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco9_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(64, self.output_channel, kernel_size=1)\n",
    "\n",
    "        self.bn1_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn1_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn2_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn2_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn3_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn3_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn4_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn4_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn5_1 = nn.BatchNorm2d(  1024)\n",
    "        self.bn5_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn6_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn6_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn7_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn7_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn8_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn8_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn9_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn9_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        h1_1 = F.relu(self.bn1_1(self.enco1_1(x)))\n",
    "        h1_2 = F.relu(self.bn1_2(self.enco1_2(h1_1)))\n",
    "        pool1, pool1_indice = F.max_pool2d(h1_2, 2, stride=2, return_indices=True) \n",
    "\n",
    "        h2_1 = F.relu(self.bn2_1(self.enco2_1(pool1)))\n",
    "        h2_2 = F.relu(self.bn2_2(self.enco2_2(h2_1)))\n",
    "        pool2, pool2_indice = F.max_pool2d(h2_2, 2, stride=2, return_indices=True)  \n",
    "\n",
    "        h3_1 = F.relu(self.bn3_1(self.enco3_1(pool2)))\n",
    "        h3_2 = F.relu(self.bn3_2(self.enco3_2(h3_1)))\n",
    "        pool3, pool3_indice = F.max_pool2d(h3_2, 2, stride=2, return_indices=True)  \n",
    "\n",
    "        h4_1 = F.relu(self.bn4_1(self.enco4_1(pool3)))\n",
    "        h4_2 = F.relu(self.bn4_2(self.enco4_2(h4_1)))\n",
    "        pool4, pool4_indice = F.max_pool2d(h4_2, 2, stride=2, return_indices=True) \n",
    "\n",
    "        h5_1 = F.relu(self.bn5_1(self.enco5_1(pool4)))\n",
    "        h5_2 = F.relu(self.bn5_2(self.enco5_2(h5_1)))\n",
    "        \n",
    "        up5 = F.max_unpool2d(h5_2, pool4_indice, kernel_size=2, stride=2, output_size=(pool3.shape[2], pool3.shape[3]))\n",
    "        h6_1 = F.relu(self.bn6_1(self.deco6_1(torch.cat((up5, h4_2), dim=1))))\n",
    "        h6_2 = F.relu(self.bn6_2(self.deco6_2(h6_1)))\n",
    "\n",
    "        up6 = F.max_unpool2d(h6_2, pool3_indice, kernel_size=2, stride=2, output_size=(pool2.shape[2], pool2.shape[3]))\n",
    "        h7_1 = F.relu(self.bn7_1(self.deco7_1(torch.cat((up6, h3_2), dim=1))))\n",
    "        h7_2 = F.relu(self.bn7_2(self.deco7_2(h7_1)))\n",
    "\n",
    "        up7 = F.max_unpool2d(h7_2, pool2_indice, kernel_size=2, stride=2, output_size=(pool1.shape[2], pool1.shape[3]))\n",
    "        h8_1 = F.relu(self.bn8_1(self.deco8_1(torch.cat((up7, h2_2), dim=1))))\n",
    "        h8_2 = F.relu(self.bn8_2(self.deco8_2(h8_1)))\n",
    "\n",
    "        up8 = F.max_unpool2d(h8_2, pool1_indice, kernel_size=2, stride=2, output_size=(x.shape[2], x.shape[3]))\n",
    "        h9_1 = F.relu(self.bn9_1(self.deco9_1(torch.cat((up8, h1_2), dim=1))))\n",
    "        h9_2 = F.relu(self.bn9_2(self.deco9_2(h9_1)))\n",
    "\n",
    "        predict = self.final_layer(h9_2)\n",
    "        \n",
    "        return torch.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c7f61-441f-4c32-8951-f42c6ad11f03",
   "metadata": {},
   "source": [
    "改めてimportされているtorch.nnとtorch.functionalには，畳み込み層やプーリング層など，U-Netを構成する部品が含まれています．使用頻度が高いので，それぞれnn, Fと簡単に記述できるようにしてあります．\n",
    "\\__init__()では UNetを構成する部品を定義し，forward()ではそれらの部品同士がどのようにデータをバケツリレーしていくかを定義します．\\\n",
    "変数名を辿るだけでも図4のネットワークがだいたい再現できていることが分かると思いますので，興味のある人は照らし合わせてみてください．\\\n",
    "BatchNorm という，図に描かれていない部品があることに気がつく人もいるかもしれません．これは，理論編で説明した ドロップアウトと並んでよく知られている正則化手法の一つで，バッチ正則化と呼ばれるものです．\\\n",
    "このように，UNetは論文で提案されたものと全く同じものが使われることは稀で，様々な工夫が追加的に施されることが一般的です．\\\n",
    "その他，個々の部品についての詳しい説明はPytorchのドキュメントや他の書籍等（こちらがオススメ）にお任せします．\n",
    "\n",
    "※UNetのようによく知られたネットワークは，世界中の誰かが既にpytorchで実装してくれているので，実際に上記のようなコードを自分で１から書かねばならないケースはほとんどありません．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af0514-45e9-44a6-a1f4-0dc379e6888b",
   "metadata": {},
   "source": [
    "### **誤差関数の定義**\n",
    "「学習」とは，所望の出力が得られるようにモデルのパラメータを調整することでした．\\\n",
    "所望の出力が得られているかどうかを判断するために重要なのは，誤差関数です．\\\n",
    "今回取り組む問題はセグメンテーションですが，その本質はピクセルごとの分類問題ですので，誤差関数としては交差エントロピーを扱うことにします（図6）．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e641ebd6-c466-4e63-9120-23f2f443385c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=1HKd0gtYMLKld5t-yGj7W1K3cOC8qAvVP\" width=\"70%\"> \n",
    "<br>\n",
    "図6　交差エントロピーの復習\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3af5e8-54cb-448f-85e3-33e6cbc3b08e",
   "metadata": {},
   "source": [
    "数式はややこしいですが，pytorchで既に実装されているものを呼び出せばいいため，準備は下記の1行で完了します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfd496-23a2-4533-9159-aaf0e473ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39115f1d-24c9-4f17-8301-f95c14252507",
   "metadata": {},
   "source": [
    "図5で説明されている交差エントロピーの定義は，3クラス以上の分類問題で利用可能なものとなっていますが，今回は肺野領域であるか否かの2値分類を行うため，Binary Cross Entropy Loss，略してBCELossと呼ばれる関数をimportします．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e2c78-6ad8-40c7-9546-59117fa8d12d",
   "metadata": {},
   "source": [
    "### **学習ループの設計**\n",
    "ここまでで，データローダ，U-Net，誤差関数の準備ができました．\\\n",
    "あとはこれらの部品を使って，学習ループを設計します．\\\n",
    "学習ループで行われるのは\\\n",
    "①データの読み込み\\\n",
    "②U-Netへの入力と順伝播計算\\\n",
    "③誤差の計算\\\n",
    "④誤差逆伝播法による勾配の計算\\\n",
    "⑤勾配降下法を用いたパラメータの更新（図6）\\\n",
    "です．①~⑤を繰り返すことにより，ニューラルネットワークの学習が進んでいきます．\n",
    "\n",
    "勾配降下法（基本的な概念は図7を参照）にはSGDやAdamなど，これまで様々なアルゴリズムが提案されてきました．今回は，近年注目を浴びているRAdamと呼ばれるアルゴリズムを使っていきます．\\\n",
    "ここではtorch.optimに実装されているRAdamを使用し，アルゴリズムの詳細については触れません．\\\n",
    "興味のある方は元論文[リンク]や解説記事[リンク]を参照してください．\n",
    "\n",
    "それでは，使用するモデルやアルゴリズム，誤差関数を改めて定義して，訓練ループを実装していきましょう．\\\n",
    "少々記述が長くなるため，細かな解説はコメントに付記していきます．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb20a8-ff4a-4227-b2e3-315b432852d9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<IMG SRC=\"https://drive.google.com/uc?id=19-O2NZYUgtodnAe1GWufNKtaEXMXoqMY\" width=\"70%\"> \n",
    "<br>\n",
    "図7　勾配降下法の復習\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9dd2a-f2bd-41a5-9cdf-1adb2b2454c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# モデルの定義\n",
    "model = UNet()\n",
    "\n",
    "# GPUを使う場合は，下記のコメントを外す\n",
    "#model = model.to(\"cuda\")\n",
    "\n",
    "# optimizer（勾配降下法のアルゴリズム）の準備\n",
    "optimizer = optim.RAdam(model.parameters())\n",
    "\n",
    "# 誤差関数の定義\n",
    "criterion = BCELoss()\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 30 #ミニバッチのサンプリングが一巡 = 1 epoch\n",
    "\n",
    "train_loss_list = [] # epoch毎のtrain_lossを保存しておくための入れ物\n",
    "val_loss_list = [] # epoch毎のvalidation_lossを保存しておくための入れ物\n",
    "\n",
    "loss_min = 100000 # validation_lossが小さくなった場合にのみモデルを保存しておくためのメモ\n",
    "\n",
    "# ここからループ開始\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss_add = 0 # 1エポック分の誤差を累積しておくための変数\n",
    "    \n",
    "    model.train() #学習モードであることを明示\n",
    "   \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        x, t = data # ①データの読み込み\n",
    "        \n",
    "        #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "        #x = x.to(\"cuda\")\n",
    "        #t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x) # ②順伝播計算\n",
    "        \n",
    "        loss = criterion(predict, t) # ③誤差の計算\n",
    "        \n",
    "        model.zero_grad()# 誤差逆伝播法のための準備\n",
    "        loss.backward() # ④誤差逆伝播法による誤差の計算\n",
    "        \n",
    "        optimizer.step() # ⑤勾配を用いてパラメータを更新\n",
    "        \n",
    "        train_loss_add += loss.data # あとで平均を計算するために，誤差を累積しておく\n",
    "        \n",
    "    train_loss_mean = train_loss_add / int(len(chest_train)/train_loader.batch_size) # 1epochでの誤差の平均を計算\n",
    "    print(\"epoch\" + str(epoch+1))\n",
    "    print(\"train_loss:\" + str(train_loss_mean))\n",
    "    train_loss_list.append(train_loss_mean.cpu())# 1epoch毎の平均を格納しておく\n",
    "    \n",
    "    # validation\n",
    "    model.eval() # 評価モード（学習を行わない）であることを明示\n",
    "    \n",
    "    val_loss_add = 0\n",
    "    for i, data in enumerate(val_loader):\n",
    "            \n",
    "        x, t = data\n",
    "        \n",
    "        #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "        #x = x.to(\"cuda\")\n",
    "        #t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x) # 順伝播計算\n",
    "        \n",
    "        loss = criterion(predict, t) # 誤差の計算\n",
    "        val_loss_add += loss.data\n",
    "        \n",
    "    val_loss_mean = val_loss_add / int(len(chest_val)/val_loader.batch_size)\n",
    "    print(\"val_loss:\" + str(val_loss_mean))\n",
    "    val_loss_list.append(val_loss_mean.cpu())\n",
    "    \n",
    "    if val_loss_mean < loss_min: # 前に保存したモデルよりもvalidation lossが小さければ，モデルを保存する\n",
    "        torch.save(model.state_dict(), \"/content/drive/MyDrive/segmentation/models/best.model\")\n",
    "        print(\"saved best model!\")\n",
    "        loss_min = val_loss_mean # 今回保存したモデルのvalidation lossをメモしておく"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9363d-2fd6-4a64-b273-e6efb922e5ec",
   "metadata": {},
   "source": [
    "### **モデルの学習過程を可視化してみよう**\n",
    "上記の学習ループでは，trainとlossをリストに格納しておきました．\\\n",
    "これをグラフ描画ライブラリmatplotlibで可視化してみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223541e5-52b8-402d-98c6-acbe6641a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list, label=\"train\") # train時のlossをplot\n",
    "plt.plot(val_loss_list, label=\"loss\") # validation時のlossをplot\n",
    "plt.xlabel(\"epoch\") # X軸のラベルを設定\n",
    "plt.ylabel(\"loss\") # Y軸のラベルを設定\n",
    "plt.legend() # 凡例を追加（plot時に指定したlabelが使われる）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfae62d-d5b3-4a18-be5f-01d1c87e0ff7",
   "metadata": {},
   "source": [
    "### セグメンテーションの精度評価\n",
    "セグメンテーションの課題では，実際に出力された画像を人目で見て確かめる「質的評価（Qualitative evaluation）」と，どれくらいきちんと抽出できているかを数値で表す「量的評価（Quantitative evaluation）」の両方を行うことが重要です．\\\n",
    "セグメンテーションの量的評価では，次のような評価指標が用いられます．\n",
    "$$ \\text{Precision} = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "$$ \\text{Recall} = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "$$ \\text{Dice Similarity Coefficient(DSC)} = \\frac{TP}{TP+\\frac{1}{2}(FP+FN)} $$\n",
    "\n",
    "$$ \\text{Intersection over Union(IoU)} = \\frac{TP}{TP+FP+FN} $$\n",
    "\n",
    "ただし，TP, TN, FP, FNは次のように定義されます．\n",
    "- True Positive(TP): 肺野として正しく見分けられたピクセルの数\n",
    "- True Negative(TN): 肺野以外として正しく見分けられたピクセルの数\n",
    "- False Positive(FP): 肺野以外を肺とみなしてしまったピクセルの数\n",
    "- False Negative(FN): 肺野を肺野以外とみなしてしまったピクセルの数\n",
    "\n",
    "Precisionは誤検知の度合いを評価し，Recallは見逃しの度合いを評価する指標であると言えます．\\\n",
    "DSCとIoUは，総合的にどれくらい上手く色塗りができたかを表す指標となりますが，IoUの方が厳しめの評価となります．\n",
    "\n",
    "それでは，testデータとして用意した画像を使って，質的評価と量的評価を行っていきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1775f61-5506-4f4c-8ea8-9ce15b413bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # TP, TN, FP, FNを求めるための機能をインポート\n",
    "\n",
    "def thresholding(inference,  threshold=0.5):\n",
    "    # U-Netが出力するのは各ピクセルに対する確率値（0~1）．\n",
    "    #ある閾値(threshold)を超えていたらそのピクセル値を255に変更する\n",
    "    \n",
    "    inference = inference.data.cpu() # 最後の一枚の１チャンネル目\n",
    "\n",
    "    mask1 = inference >= threshold\n",
    "    inference[mask1] = 255\n",
    "\n",
    "    mask0 = inference < threshold\n",
    "    inference[mask0] = 0\n",
    "    \n",
    "    return inference\n",
    "\n",
    "def calc_all(tp, pp):\n",
    "    # TP, TN, FP, FN, Accuracy, Precision, Recall, DSC, IoUを計算する関数\n",
    "    \n",
    "    mask = pp != 0\n",
    "    pp[mask] = 1\n",
    "    tn, fp, fn, tp = confusion_matrix(tp.flatten(), pp.flatten()).ravel() # TP, TN, FP, FNを計算\n",
    "    presicion = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    dice = tp / (tp + ((1/2)*(fp+fn)))\n",
    "    iou = tp / (tp + fp + fn)\n",
    "    return presicion, recall, dice, iou\n",
    "\n",
    "\n",
    "model = UNet(input_channel=1, output_channel=1) # テストに使うためのU-Netを改めて定義\n",
    "\n",
    "#GPU環境で動かす際は，下記のコメントを外す\n",
    "#model = model.to(\"cuda\")\n",
    "\n",
    "#高屋が予め用意した学習済みモデルを使う場合は，best.modelをtakaya.modelに書き換えてください\n",
    "model_path = \"/content/drive/MyDrive/segmentation/models/best.model\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path)) # 学習時に保存しておいたモデルのパラメータをコピー\n",
    "\n",
    "model.eval() # 評価モードであることを明示\n",
    "\n",
    "# 各画像のPrecision, Recall, Dice, IoUを保存しておくためのリスト\n",
    "precision_list = [] \n",
    "recall_list = []\n",
    "dice_list = []\n",
    "iou_list = []\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    \n",
    "    x, t = data\n",
    "    \n",
    "    #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "    #x = x.to(\"cuda\")\n",
    "    #t = t.to(\"cuda\")\n",
    "        \n",
    "    predict = model(x) # 学習済みU-Netによる推論\n",
    "    predict_imgs = thresholding(predict, threshold=0.5) #出力された確率値（0~1）を8bitに(0~255)に変換\n",
    "    \n",
    "    for j in range(test_loader.batch_size):\n",
    "        img = np.array(predict_imgs[j][0]) # 1チャンネル目だけ取り出して，(256, 256)の形にする\n",
    "        xx = np.array(x[j][0].cpu()) # 入力画像を表示するための準備（numpy行列への変換）\n",
    "        tt = np.array(t[j][0].cpu()) # ラベルを表示したり，指標を計算するための準備（numpy行列への変換）\n",
    "        \n",
    "        precision, recall, dice, iou = calc_all(img/255, tt) # 評価指標の計算\n",
    "        \n",
    "        #計算された指標を各リストへ格納\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        dice_list.append(dice)\n",
    "        iou_list.append(iou)\n",
    "        \n",
    "        # 質的評価のための画像出力（入力画像，ラベル，出力画像を並べて表示したい）\n",
    "        plt.subplot(1, 3, 1)#1桁目 -- グラフの行数、2桁目 -- グラフの列数、3桁目 -- グラフの番号、subplot(2,3,1)の記載でも良い。\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"input\")\n",
    "        plt.imshow(xx, cmap = \"gray\")\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"target\")\n",
    "        plt.imshow(tt, cmap = \"gray\")\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"output\\n(iou=\" + str(round(iou, 4)) + \")\") #小数第4位までのIoUを図の上に表示\n",
    "        plt.imshow(img, cmap = \"gray\")\n",
    "        plt.savefig(\"/content/drive/MyDrive/segmentation/results/results/\" + str(i) + \"_\" + str(j) + \".png\")\n",
    "\n",
    "precision_list = np.array(precision_list)\n",
    "recall_list = np.array(recall_list)\n",
    "dice_list = np.array(dice_list)\n",
    "iou_list = np.array(iou_list)\n",
    "\n",
    "precision_mean = np.array(precision_list).mean()\n",
    "recall_mean = np.array(recall_list).mean()\n",
    "dice_mean = np.array(dice_list).mean()\n",
    "iou_mean = np.array(iou_list).mean()\n",
    "\n",
    "# 各種指標を表示\n",
    "print(\"Precision: \" + str(precision_mean))\n",
    "print(\"Recall: \" + str(recall_mean))\n",
    "print(\"DSC: \" + str(dice_mean))\n",
    "print(\"IoU: \" + str(iou_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b3753-86a4-447b-bded-9841572d8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7f695-23ef-422e-83b4-cc686243cb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
