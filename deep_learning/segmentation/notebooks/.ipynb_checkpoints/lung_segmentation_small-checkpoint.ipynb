{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3625ef1-89dc-4a27-b3b9-f20f61e9acac",
   "metadata": {},
   "source": [
    "# より少ないデータで試してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310d6c7-1ff4-4282-b5ce-38628ed00b85",
   "metadata": {},
   "source": [
    "### 問題設定\n",
    "トレーニング用のデータが5枚しかない状況を想定してみましょう．\n",
    "ただし，val, testはこれまでと同様の枚数とします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25751e03-9d58-4553-9e48-e5619d0f36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, TrainValTest=\"train\"):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        どのデータを使うのかを記述する部分\n",
    "        \"\"\"\n",
    "        if TrainValTest == \"train\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/small_train/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/small_train/label/*\"))\n",
    "        elif TrainValTest == \"val\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/val/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/val/label/*\"))\n",
    "        elif TrainValTest == \"test\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/test/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/content/drive/MyDrive/segmentation/data/test/label/*\"))    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        データがいくつあるのかを数える\n",
    "        \"\"\"\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        データをどのような形で取り出すのか記述する\n",
    "        \"\"\"\n",
    "        image_path = self.img_path_list[index] # ファイル名\n",
    "        label_path = self.label_path_list[index] # ファイル名\n",
    "        \n",
    "        img = Image.open(image_path) # ファイル名を与えて画像を取り出す\n",
    "        img = np.array(img) # 画像をnumpy形式の行列へ変換\n",
    "        img = np.expand_dims(img, 0) # 1チャンネルであることを明示する（256, 256）→ (1, 256, 256)\n",
    "        img = torch.tensor(img) # 行列をpytorchで扱える形式（tensor型）に変換する\n",
    "        img = img / 255 # 0~255までの値を0~1までの値に変換する\n",
    "        \n",
    "        label = Image.open(label_path) # ファイル名を与えてラベルを取り出す\n",
    "        label = np.array(label) # ラベルをnumpy形式の行列へ変換\n",
    "        label = np.expand_dims(label, 0)\n",
    "        label = torch.tensor(label)\n",
    "        label = label = label / 255 # 肺野領域は255，それ以外は0となっているので，255で割って0 or 1に変換する\n",
    "        label = label.float() # 行列の値をfloat型に変換する（pytorchの都合）\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bb92a-7c83-48ad-9c62-5afa090e65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_train = ChestDataset(TrainValTest=\"train\")\n",
    "chest_val = ChestDataset(TrainValTest=\"val\")\n",
    "chest_test = ChestDataset(TrainValTest=\"test\")\n",
    "\n",
    "train_loader = DataLoader(chest_train, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(chest_val, batch_size=5, shuffle=False)\n",
    "test_loader = DataLoader(chest_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8e3a5-a05d-4d8a-8bbf-0f73b081a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=2, input_channel=1, output_channel=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.enco1_1 = nn.Conv2d(self.input_channel, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.enco2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco5_1 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco5_2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco6_1 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco6_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco7_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco7_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco8_1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco8_2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco9_1 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco9_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(64, self.output_channel, kernel_size=1)\n",
    "\n",
    "        self.bn1_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn1_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn2_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn2_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn3_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn3_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn4_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn4_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn5_1 = nn.BatchNorm2d(  1024)\n",
    "        self.bn5_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn6_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn6_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn7_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn7_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn8_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn8_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn9_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn9_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        h1_1 = F.relu(self.bn1_1(self.enco1_1(x)))\n",
    "        h1_2 = F.relu(self.bn1_2(self.enco1_2(h1_1)))\n",
    "        pool1, pool1_indice = F.max_pool2d(h1_2, 2, stride=2, return_indices=True) \n",
    "\n",
    "        h2_1 = F.relu(self.bn2_1(self.enco2_1(pool1)))\n",
    "        h2_2 = F.relu(self.bn2_2(self.enco2_2(h2_1)))\n",
    "        pool2, pool2_indice = F.max_pool2d(h2_2, 2, stride=2, return_indices=True)  \n",
    "\n",
    "        h3_1 = F.relu(self.bn3_1(self.enco3_1(pool2)))\n",
    "        h3_2 = F.relu(self.bn3_2(self.enco3_2(h3_1)))\n",
    "        pool3, pool3_indice = F.max_pool2d(h3_2, 2, stride=2, return_indices=True)  \n",
    "\n",
    "        h4_1 = F.relu(self.bn4_1(self.enco4_1(pool3)))\n",
    "        h4_2 = F.relu(self.bn4_2(self.enco4_2(h4_1)))\n",
    "        pool4, pool4_indice = F.max_pool2d(h4_2, 2, stride=2, return_indices=True) \n",
    "\n",
    "        h5_1 = F.relu(self.bn5_1(self.enco5_1(pool4)))\n",
    "        h5_2 = F.relu(self.bn5_2(self.enco5_2(h5_1)))\n",
    "        \n",
    "        up5 = F.max_unpool2d(h5_2, pool4_indice, kernel_size=2, stride=2, output_size=(pool3.shape[2], pool3.shape[3]))\n",
    "        h6_1 = F.relu(self.bn6_1(self.deco6_1(torch.cat((up5, h4_2), dim=1))))\n",
    "        h6_2 = F.relu(self.bn6_2(self.deco6_2(h6_1)))\n",
    "\n",
    "        up6 = F.max_unpool2d(h6_2, pool3_indice, kernel_size=2, stride=2, output_size=(pool2.shape[2], pool2.shape[3]))\n",
    "        h7_1 = F.relu(self.bn7_1(self.deco7_1(torch.cat((up6, h3_2), dim=1))))\n",
    "        h7_2 = F.relu(self.bn7_2(self.deco7_2(h7_1)))\n",
    "\n",
    "        up7 = F.max_unpool2d(h7_2, pool2_indice, kernel_size=2, stride=2, output_size=(pool1.shape[2], pool1.shape[3]))\n",
    "        h8_1 = F.relu(self.bn8_1(self.deco8_1(torch.cat((up7, h2_2), dim=1))))\n",
    "        h8_2 = F.relu(self.bn8_2(self.deco8_2(h8_1)))\n",
    "\n",
    "        up8 = F.max_unpool2d(h8_2, pool1_indice, kernel_size=2, stride=2, output_size=(x.shape[2], x.shape[3]))\n",
    "        h9_1 = F.relu(self.bn9_1(self.deco9_1(torch.cat((up8, h1_2), dim=1))))\n",
    "        h9_2 = F.relu(self.bn9_2(self.deco9_2(h9_1)))\n",
    "\n",
    "        predict = self.final_layer(h9_2)\n",
    "        \n",
    "\n",
    "        return torch.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e55a91-1a80-4ecb-9543-6f1181133879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "import torch.optim as optim\n",
    "\n",
    "# モデルの定義\n",
    "model = UNet()\n",
    "\n",
    "# GPUを使う場合は，下記のコメントを外す\n",
    "#model = model.to(\"cuda\")\n",
    "\n",
    "# optimizer（勾配降下法のアルゴリズム）の準備\n",
    "optimizer = optim.RAdam(model.parameters())\n",
    "\n",
    "# 誤差関数の定義\n",
    "criterion = BCELoss()\n",
    "\n",
    "# 学習ループ\n",
    "epochs = 100 #ミニバッチのサンプリングが一巡 = 1 epoch\n",
    "\n",
    "train_loss_list = [] # epoch毎のtrain_lossを保存しておくための入れ物\n",
    "val_loss_list = [] # epoch毎のvalidation_lossを保存しておくための入れ物\n",
    "\n",
    "loss_min = 100000 # validation_lossが小さくなった場合にのみモデルを保存しておくためのメモ\n",
    "\n",
    "# ここからループ開始\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss_add = 0 # 1エポック分の誤差を累積しておくための変数\n",
    "    \n",
    "    model.train() #学習モードであることを明示\n",
    "   \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        x, t = data # ①データの読み込み\n",
    "        \n",
    "        #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "        #x = x.to(\"cuda\")\n",
    "        #t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x) # ②順伝播計算\n",
    "        \n",
    "        loss = criterion(predict, t) # ③誤差の計算\n",
    "        \n",
    "        model.zero_grad()# 誤差逆伝播法のための準備\n",
    "        loss.backward() # ④誤差逆伝播法による誤差の計算\n",
    "        \n",
    "        optimizer.step() # ⑤勾配を用いてパラメータを更新\n",
    "        \n",
    "        train_loss_add += loss.data # あとで平均を計算するために，誤差を累積しておく\n",
    "        \n",
    "    train_loss_mean = train_loss_add / int(len(chest_train)/train_loader.batch_size) # 1epochでの誤差の平均を計算\n",
    "    print(\"epoch\" + str(epoch+1))\n",
    "    print(\"train_loss:\" + str(train_loss_mean))\n",
    "    train_loss_list.append(train_loss_mean.cpu())# 1epoch毎の平均を格納しておく\n",
    "    \n",
    "    # validation\n",
    "    model.eval() # 評価モード（学習を行わない）であることを明示\n",
    "    \n",
    "    val_loss_add = 0\n",
    "    for i, data in enumerate(val_loader):\n",
    "            \n",
    "        x, t = data\n",
    "        \n",
    "        #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "        #x = x.to(\"cuda\")\n",
    "        #t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x) # 順伝播計算\n",
    "        \n",
    "        loss = criterion(predict, t) # 誤差の計算\n",
    "        val_loss_add += loss.data\n",
    "        \n",
    "    val_loss_mean = val_loss_add / int(len(chest_val)/val_loader.batch_size)\n",
    "    print(\"val_loss:\" + str(val_loss_mean))\n",
    "    val_loss_list.append(val_loss_mean.cpu())\n",
    "    \n",
    "    if val_loss_mean < loss_min: # 前に保存したモデルよりもvalidation lossが小さければ，モデルを保存する\n",
    "        torch.save(model.state_dict(), \"/content/drive/MyDrive/segmentation/models/small_best.model\")\n",
    "        print(\"saved best model!\")\n",
    "        loss_min = val_loss_mean # 今回保存したモデルのvalidation lossをメモしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d469ec-d03e-41e9-b3c4-0858ac083ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list, label=\"train\") # train時のlossをplot\n",
    "plt.plot(val_loss_list, label=\"loss\") # validation時のlossをplot\n",
    "plt.xlabel(\"epoch\") # X軸のラベルを設定\n",
    "plt.ylabel(\"loss\") # Y軸のラベルを設定\n",
    "plt.legend() # 凡例を追加（plot時に指定したlabelが使われる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc2078-f2bb-432a-9cf5-6034ad2e3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # TP, TN, FP, FNを求めるための機能をインポート\n",
    "\n",
    "def thresholding(inference,  threshold=0.5):\n",
    "    # U-Netが出力するのは各ピクセルに対する確率値（0~1）．\n",
    "    #ある閾値(threshold)を超えていたらそのピクセル値を255に変更する\n",
    "    \n",
    "    inference = inference.data.cpu() # 最後の一枚の１チャンネル目\n",
    "\n",
    "    mask1 = inference >= threshold\n",
    "    inference[mask1] = 255\n",
    "\n",
    "    mask0 = inference < threshold\n",
    "    inference[mask0] = 0\n",
    "    \n",
    "    return inference\n",
    "\n",
    "def calc_all(tp, pp):\n",
    "    # TP, TN, FP, FN, Accuracy, Precision, Recall, DSC, IoUを計算する関数\n",
    "    \n",
    "    mask = pp != 0\n",
    "    pp[mask] = 1\n",
    "    tn, fp, fn, tp = confusion_matrix(tp.flatten(), pp.flatten()).ravel() # TP, TN, FP, FNを計算\n",
    "    presicion = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    dice = tp / (tp + ((1/2)*(fp+fn)))\n",
    "    iou = tp / (tp + fp + fn)\n",
    "    return presicion, recall, dice, iou\n",
    "\n",
    "\n",
    "model = UNet(input_channel=1, output_channel=1) # テストに使うためのU-Netを改めて定義\n",
    "\n",
    "#GPU環境で動かす際は，下記のコメントを外す\n",
    "#model = model.to(\"cuda\")\n",
    "\n",
    "#高屋が予め用意した学習済みモデルを使う場合は，small_best.modelをsmall_takaya.modelに書き換えてください\n",
    "model_path = \"/content/drive/MyDrive/segmentation/models/small_takaya.model\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path)) # 学習時に保存しておいたモデルのパラメータをコピー\n",
    "\n",
    "model.eval() # 評価モードであることを明示\n",
    "\n",
    "# 各画像のPrecision, Recall, Dice, IoUを保存しておくためのリスト\n",
    "precision_list = [] \n",
    "recall_list = []\n",
    "dice_list = []\n",
    "iou_list = []\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    \n",
    "    x, t = data\n",
    "    \n",
    "    #GPU環境で動かす際は，下記2行のコメントを外す\n",
    "    #x = x.to(\"cuda\")\n",
    "    #t = t.to(\"cuda\")\n",
    "        \n",
    "    predict = model(x) # 学習済みU-Netによる推論\n",
    "    predict_imgs = thresholding(predict, threshold=0.5) #出力された確率値（0~1）を8bitに(0~255)に変換\n",
    "    \n",
    "    for j in range(test_loader.batch_size):\n",
    "        img = np.array(predict_imgs[j][0]) # 1チャンネル目だけ取り出して，(256, 256)の形にする\n",
    "        xx = np.array(x[j][0].cpu()) # 入力画像を表示するための準備（numpy行列への変換）\n",
    "        tt = np.array(t[j][0].cpu()) # ラベルを表示したり，指標を計算するための準備（numpy行列への変換）\n",
    "        \n",
    "        precision, recall, dice, iou = calc_all(img/255, tt) # 評価指標の計算\n",
    "        \n",
    "        #計算された指標を各リストへ格納\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        dice_list.append(dice)\n",
    "        iou_list.append(iou)\n",
    "        \n",
    "        # 質的評価のための画像出力（入力画像，ラベル，出力画像を並べて表示したい）\n",
    "        plt.subplot(1, 3, 1)#1桁目 -- グラフの行数、2桁目 -- グラフの列数、3桁目 -- グラフの番号、subplot(2,3,1)の記載でも良い。\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"input\")\n",
    "        plt.imshow(xx, cmap = \"gray\")\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"target\")\n",
    "        plt.imshow(tt, cmap = \"gray\")\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.axis(\"off\") # 軸のメモリを表示しない\n",
    "        plt.title(\"output\\n(iou=\" + str(round(iou, 4)) + \")\") #小数第4位までのIoUを図の上に表示\n",
    "        plt.imshow(img, cmap = \"gray\")\n",
    "        plt.savefig(\"/content/drive/MyDrive/segmentation/results/small_results/\" + str(i) + \"_\" + str(j) + \".png\")\n",
    "\n",
    "precision_list = np.array(precision_list)\n",
    "recall_list = np.array(recall_list)\n",
    "dice_list = np.array(dice_list)\n",
    "iou_list = np.array(iou_list)\n",
    "\n",
    "precision_mean = np.array(precision_list).mean()\n",
    "recall_mean = np.array(recall_list).mean()\n",
    "dice_mean = np.array(dice_list).mean()\n",
    "iou_mean = np.array(iou_list).mean()\n",
    "\n",
    "# 各種指標を表示\n",
    "print(\"Precision: \" + str(precision_mean))\n",
    "print(\"Recall: \" + str(recall_mean))\n",
    "print(\"DSC: \" + str(dice_mean))\n",
    "print(\"IoU: \" + str(iou_mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
