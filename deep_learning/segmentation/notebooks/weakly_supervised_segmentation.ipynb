{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3625ef1-89dc-4a27-b3b9-f20f61e9acac",
   "metadata": {},
   "source": [
    "# 弱教師あり学習をやってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78d60b-ddbf-4100-91d0-98e005cd0fa7",
   "metadata": {},
   "source": [
    "## 弱教師あり学習のおさらい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549500d-daf3-451e-ad19-9ab1db6de8dd",
   "metadata": {},
   "source": [
    "## 弱教師あり学習による肺セグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310d6c7-1ff4-4282-b5ce-38628ed00b85",
   "metadata": {},
   "source": [
    "### 問題設定\n",
    "~の論文を参考に，弱教師ありセグメンテーションに挑戦しましょう．\\\n",
    "ここでの「弱教師」は，アノテーションを省エネで付与した場合を考えます．\\\n",
    "具体的には，肺領域をかなり雑に塗ったようなラベルが得られているとします（図１）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25751e03-9d58-4553-9e48-e5619d0f36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, TrainValTest=\"train\", shuffle=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if TrainValTest == \"train\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/train/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/train/weak_label/*\"))\n",
    "        elif TrainValTest == \"val\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/val/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/val/label/*\"))\n",
    "        elif TrainValTest == \"test\":\n",
    "            self.img_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/test/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/test/label/*\"))\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.img_path_list[index]\n",
    "        label_path = self.label_path_list[index]\n",
    "        img = np.array(Image.open(image_path))\n",
    "        img = self.add_axis(img)\n",
    "        label = np.array(Image.open(label_path))\n",
    "        label = self._label_transform(label)\n",
    "        label = self.add_axis(label)\n",
    "        return img, label\n",
    "    \n",
    "    def _label_transform(self, label):# transformの関数に書き換える\n",
    "        nonzero = np.nonzero(label)\n",
    "        label[nonzero] = 1\n",
    "        return label\n",
    "    \n",
    "    def add_axis(self, arr):# transformの関数に書き換える\n",
    "        arr_new = np.expand_dims(arr, 0)\n",
    "        return arr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a0bb92a-7c83-48ad-9c62-5afa090e65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_train = ChestDataset(TrainValTest=\"train\", shuffle=True)\n",
    "chest_val = ChestDataset(TrainValTest=\"val\", shuffle=False)\n",
    "chest_test = ChestDataset(TrainValTest=\"test\", shuffle=False)\n",
    "train_loader = DataLoader(chest_train, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(chest_val, batch_size=5, shuffle=False)\n",
    "test_loader = DataLoader(chest_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a8e3a5-a05d-4d8a-8bbf-0f73b081a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=2, input_channel=1, output_channel=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.enco1_1 = nn.Conv2d(self.input_channel, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.enco2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco5_1 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco5_2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco6_1 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco6_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco7_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco7_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco8_1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco8_2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco9_1 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco9_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(64, self.output_channel, kernel_size=1)\n",
    "\n",
    "        self.bn1_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn1_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn2_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn2_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn3_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn3_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn4_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn4_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn5_1 = nn.BatchNorm2d(  1024)\n",
    "        self.bn5_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn6_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn6_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn7_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn7_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn8_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn8_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn9_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn9_2 = nn.BatchNorm2d(  64)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  \n",
    "\n",
    "    def forward(self, x): #x = (batchsize, 3, 360, 480)\n",
    "        #if LRN:\n",
    "        #    x = F.local_response_normalization(x) #Needed for preventing from overfitting\n",
    "\n",
    "        h1_1 = F.relu(self.bn1_1(self.enco1_1(x)))\n",
    "        h1_2 = F.relu(self.bn1_2(self.enco1_2(h1_1)))\n",
    "        pool1, pool1_indice = F.max_pool2d(h1_2, 2, stride=2, return_indices=True) #(batchsize,  64, 180, 240)\n",
    "\n",
    "        h2_1 = F.relu(self.bn2_1(self.enco2_1(pool1)))\n",
    "        h2_2 = F.relu(self.bn2_2(self.enco2_2(h2_1)))\n",
    "        pool2, pool2_indice = F.max_pool2d(h2_2, 2, stride=2, return_indices=True) #(batchsize, 128,  90, 120) \n",
    "\n",
    "        h3_1 = F.relu(self.bn3_1(self.enco3_1(pool2)))\n",
    "        h3_2 = F.relu(self.bn3_2(self.enco3_2(h3_1)))\n",
    "        pool3, pool3_indice = F.max_pool2d(h3_2, 2, stride=2, return_indices=True) #(batchsize, 256,  45,  60) \n",
    "\n",
    "        h4_1 = F.relu(self.bn4_1(self.enco4_1(pool3)))\n",
    "        h4_2 = F.relu(self.bn4_2(self.enco4_2(h4_1)))\n",
    "        pool4, pool4_indice = F.max_pool2d(h4_2, 2, stride=2, return_indices=True) #(batchsize, 256,  23,  30) \n",
    "\n",
    "        h5_1 = F.relu(self.bn5_1(self.enco5_1(pool4)))\n",
    "        h5_2 = F.relu(self.bn5_2(self.enco5_2(h5_1)))\n",
    "        \n",
    "        up5 = F.max_unpool2d(h5_2, pool4_indice, kernel_size=2, stride=2, output_size=(pool3.shape[2], pool3.shape[3]))\n",
    "        h6_1 = F.relu(self.bn6_1(self.deco6_1(torch.cat((up5, h4_2), dim=1))))\n",
    "        h6_2 = F.relu(self.bn6_2(self.deco6_2(h6_1)))\n",
    "\n",
    "        up6 = F.max_unpool2d(h6_2, pool3_indice, kernel_size=2, stride=2, output_size=(pool2.shape[2], pool2.shape[3]))\n",
    "        h7_1 = F.relu(self.bn7_1(self.deco7_1(torch.cat((up6, h3_2), dim=1))))\n",
    "        h7_2 = F.relu(self.bn7_2(self.deco7_2(h7_1)))\n",
    "\n",
    "        up7 = F.max_unpool2d(h7_2, pool2_indice, kernel_size=2, stride=2, output_size=(pool1.shape[2], pool1.shape[3]))\n",
    "        h8_1 = F.relu(self.bn8_1(self.deco8_1(torch.cat((up7, h2_2), dim=1))))\n",
    "        h8_2 = F.relu(self.bn8_2(self.deco8_2(h8_1)))\n",
    "\n",
    "        up8 = F.max_unpool2d(h8_2, pool1_indice, kernel_size=2, stride=2, output_size=(x.shape[2], x.shape[3])) #x = (batchsize, 128, 360, 480)\n",
    "        h9_1 = F.relu(self.bn9_1(self.deco9_1(torch.cat((up8, h1_2), dim=1))))\n",
    "        h9_2 = F.relu(self.bn9_2(self.deco9_2(h9_1)))\n",
    "\n",
    "        h = self.final_layer(h9_2)\n",
    "        #print(h.shape)\n",
    "        #print(t.shape)\n",
    "        predict = h\n",
    "        #loss = \tnn.BCEWithLogitsLoss(h, t)\n",
    "        \n",
    "        #predict = nn.Softmax(h)\n",
    "        return torch.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4b172d-fc55-4a94-8f05-b0b5f2b50eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "from torch import einsum\n",
    "\n",
    "\n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target)\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    if input.is_cuda:\n",
    "        s = torch.FloatTensor(1).cuda().zero_()\n",
    "    else:\n",
    "        s = torch.FloatTensor(1).zero_()\n",
    "\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "\n",
    "    return s / (i + 1)\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
    "        smooth = 1e-5\n",
    "        input = torch.sigmoid(input)\n",
    "        num = target.size(0)\n",
    "        input = input.view(num, -1)\n",
    "        target = target.view(num, -1)\n",
    "        intersection = (input * target)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "        return 0.5 * bce + dice\n",
    "\n",
    "class NaiveSizeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This one implement the naive quadratic penalty\n",
    "    penalty = 0                  if a <= pred_size\n",
    "              (a - pred_size)^2  otherwise\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NaiveSizeLoss, self).__init__()\n",
    "\n",
    "    def __call__(self, pred_softmax, bounds):\n",
    "        #assert simplex(pred_softmax)\n",
    "\n",
    "        B, K, H, W = pred_softmax.shape\n",
    "        #assert bounds.shape == (B, K, 2)\n",
    "\n",
    "        pred_size = einsum(\"bkwh->bk\", pred_softmax)\n",
    "        #print(pred_size)\n",
    "        upper_bounds = bounds[1]\n",
    "        lower_bounds = bounds[0]\n",
    "        assert (upper_bounds >= 0).all() and (lower_bounds >= 0).all()\n",
    "\n",
    "        # size < upper <==> size - upper < 0\n",
    "        # lower < size <==> lower - size < 0\n",
    "        \n",
    "        loss = F.relu(pred_size - upper_bounds) ** 2 + F.relu(lower_bounds - pred_size) ** 2\n",
    "        loss /= (W * H)\n",
    "\n",
    "        return loss.sum() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e55a91-1a80-4ecb-9543-6f1181133879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8127/1597229496.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_8127/1597229496.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "train_loss:tensor(0.5734, device='cuda:0')\n",
      "val_loss:tensor(0.6603, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8127/1597229496.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_8127/1597229496.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model!\n",
      "epoch2\n",
      "train_loss:tensor(0.5253, device='cuda:0')\n",
      "val_loss:tensor(0.6478, device='cuda:0')\n",
      "saved best model!\n",
      "epoch3\n",
      "train_loss:tensor(0.5113, device='cuda:0')\n",
      "val_loss:tensor(0.6219, device='cuda:0')\n",
      "saved best model!\n",
      "epoch4\n",
      "train_loss:tensor(0.4884, device='cuda:0')\n",
      "val_loss:tensor(0.5988, device='cuda:0')\n",
      "saved best model!\n",
      "epoch5\n",
      "train_loss:tensor(0.4712, device='cuda:0')\n",
      "val_loss:tensor(0.5894, device='cuda:0')\n",
      "saved best model!\n",
      "epoch6\n",
      "train_loss:tensor(0.4832, device='cuda:0')\n",
      "val_loss:tensor(0.6042, device='cuda:0')\n",
      "epoch7\n",
      "train_loss:tensor(0.4765, device='cuda:0')\n",
      "val_loss:tensor(0.5379, device='cuda:0')\n",
      "saved best model!\n",
      "epoch8\n",
      "train_loss:tensor(0.5070, device='cuda:0')\n",
      "val_loss:tensor(0.5927, device='cuda:0')\n",
      "epoch9\n",
      "train_loss:tensor(0.4957, device='cuda:0')\n",
      "val_loss:tensor(0.6356, device='cuda:0')\n",
      "epoch10\n",
      "train_loss:tensor(0.5539, device='cuda:0')\n",
      "val_loss:tensor(0.6351, device='cuda:0')\n",
      "epoch11\n",
      "train_loss:tensor(0.5438, device='cuda:0')\n",
      "val_loss:tensor(0.6468, device='cuda:0')\n",
      "epoch12\n",
      "train_loss:tensor(0.5277, device='cuda:0')\n",
      "val_loss:tensor(0.5913, device='cuda:0')\n",
      "epoch13\n",
      "train_loss:tensor(0.5154, device='cuda:0')\n",
      "val_loss:tensor(0.5862, device='cuda:0')\n",
      "epoch14\n",
      "train_loss:tensor(0.5072, device='cuda:0')\n",
      "val_loss:tensor(0.5664, device='cuda:0')\n",
      "epoch15\n",
      "train_loss:tensor(0.4999, device='cuda:0')\n",
      "val_loss:tensor(0.5610, device='cuda:0')\n",
      "epoch16\n",
      "train_loss:tensor(0.5033, device='cuda:0')\n",
      "val_loss:tensor(0.5579, device='cuda:0')\n",
      "epoch17\n",
      "train_loss:tensor(0.5039, device='cuda:0')\n",
      "val_loss:tensor(0.5678, device='cuda:0')\n",
      "epoch18\n",
      "train_loss:tensor(0.5004, device='cuda:0')\n",
      "val_loss:tensor(0.5614, device='cuda:0')\n",
      "epoch19\n",
      "train_loss:tensor(0.4944, device='cuda:0')\n",
      "val_loss:tensor(0.5322, device='cuda:0')\n",
      "saved best model!\n",
      "epoch20\n",
      "train_loss:tensor(0.4829, device='cuda:0')\n",
      "val_loss:tensor(0.5173, device='cuda:0')\n",
      "saved best model!\n",
      "epoch21\n",
      "train_loss:tensor(0.4827, device='cuda:0')\n",
      "val_loss:tensor(0.6514, device='cuda:0')\n",
      "epoch22\n",
      "train_loss:tensor(0.5264, device='cuda:0')\n",
      "val_loss:tensor(0.6219, device='cuda:0')\n",
      "epoch23\n",
      "train_loss:tensor(0.5299, device='cuda:0')\n",
      "val_loss:tensor(0.6028, device='cuda:0')\n",
      "epoch24\n",
      "train_loss:tensor(0.5069, device='cuda:0')\n",
      "val_loss:tensor(0.5807, device='cuda:0')\n",
      "epoch25\n",
      "train_loss:tensor(0.5079, device='cuda:0')\n",
      "val_loss:tensor(0.5814, device='cuda:0')\n",
      "epoch26\n",
      "train_loss:tensor(0.5080, device='cuda:0')\n",
      "val_loss:tensor(0.5753, device='cuda:0')\n",
      "epoch27\n",
      "train_loss:tensor(0.4998, device='cuda:0')\n",
      "val_loss:tensor(0.5675, device='cuda:0')\n",
      "epoch28\n",
      "train_loss:tensor(0.4933, device='cuda:0')\n",
      "val_loss:tensor(0.5529, device='cuda:0')\n",
      "epoch29\n",
      "train_loss:tensor(0.4976, device='cuda:0')\n",
      "val_loss:tensor(0.5401, device='cuda:0')\n",
      "epoch30\n",
      "train_loss:tensor(0.4955, device='cuda:0')\n",
      "val_loss:tensor(0.5558, device='cuda:0')\n",
      "epoch31\n",
      "train_loss:tensor(0.4811, device='cuda:0')\n",
      "val_loss:tensor(0.5405, device='cuda:0')\n",
      "epoch32\n",
      "train_loss:tensor(0.4881, device='cuda:0')\n",
      "val_loss:tensor(0.5602, device='cuda:0')\n",
      "epoch33\n",
      "train_loss:tensor(0.4978, device='cuda:0')\n",
      "val_loss:tensor(0.5289, device='cuda:0')\n",
      "epoch34\n",
      "train_loss:tensor(0.4851, device='cuda:0')\n",
      "val_loss:tensor(0.5276, device='cuda:0')\n",
      "epoch35\n",
      "train_loss:tensor(0.4850, device='cuda:0')\n",
      "val_loss:tensor(0.5426, device='cuda:0')\n",
      "epoch36\n",
      "train_loss:tensor(0.4935, device='cuda:0')\n",
      "val_loss:tensor(0.5326, device='cuda:0')\n",
      "epoch37\n",
      "train_loss:tensor(0.4893, device='cuda:0')\n",
      "val_loss:tensor(0.5376, device='cuda:0')\n",
      "epoch38\n",
      "train_loss:tensor(0.4663, device='cuda:0')\n",
      "val_loss:tensor(0.5788, device='cuda:0')\n",
      "epoch39\n",
      "train_loss:tensor(0.4925, device='cuda:0')\n",
      "val_loss:tensor(0.5909, device='cuda:0')\n",
      "epoch40\n",
      "train_loss:tensor(0.4919, device='cuda:0')\n",
      "val_loss:tensor(0.5446, device='cuda:0')\n",
      "epoch41\n",
      "train_loss:tensor(0.5194, device='cuda:0')\n",
      "val_loss:tensor(0.5445, device='cuda:0')\n",
      "epoch42\n",
      "train_loss:tensor(0.5574, device='cuda:0')\n",
      "val_loss:tensor(0.5712, device='cuda:0')\n",
      "epoch43\n",
      "train_loss:tensor(0.5256, device='cuda:0')\n",
      "val_loss:tensor(0.5743, device='cuda:0')\n",
      "epoch44\n",
      "train_loss:tensor(0.5245, device='cuda:0')\n",
      "val_loss:tensor(0.5642, device='cuda:0')\n",
      "epoch45\n",
      "train_loss:tensor(0.5170, device='cuda:0')\n",
      "val_loss:tensor(0.5677, device='cuda:0')\n",
      "epoch46\n",
      "train_loss:tensor(0.5132, device='cuda:0')\n",
      "val_loss:tensor(0.5741, device='cuda:0')\n",
      "epoch47\n",
      "train_loss:tensor(0.5097, device='cuda:0')\n",
      "val_loss:tensor(0.5817, device='cuda:0')\n",
      "epoch48\n",
      "train_loss:tensor(0.5045, device='cuda:0')\n",
      "val_loss:tensor(0.5885, device='cuda:0')\n",
      "epoch49\n",
      "train_loss:tensor(0.5319, device='cuda:0')\n",
      "val_loss:tensor(0.6173, device='cuda:0')\n",
      "epoch50\n",
      "train_loss:tensor(0.5309, device='cuda:0')\n",
      "val_loss:tensor(0.6146, device='cuda:0')\n",
      "epoch51\n",
      "train_loss:tensor(0.5170, device='cuda:0')\n",
      "val_loss:tensor(0.5961, device='cuda:0')\n",
      "epoch52\n",
      "train_loss:tensor(0.4938, device='cuda:0')\n",
      "val_loss:tensor(0.5862, device='cuda:0')\n",
      "epoch53\n",
      "train_loss:tensor(0.4815, device='cuda:0')\n",
      "val_loss:tensor(0.5639, device='cuda:0')\n",
      "epoch54\n",
      "train_loss:tensor(0.4736, device='cuda:0')\n",
      "val_loss:tensor(0.5584, device='cuda:0')\n",
      "epoch55\n",
      "train_loss:tensor(0.4664, device='cuda:0')\n",
      "val_loss:tensor(0.5627, device='cuda:0')\n",
      "epoch56\n",
      "train_loss:tensor(0.4631, device='cuda:0')\n",
      "val_loss:tensor(0.5596, device='cuda:0')\n",
      "epoch57\n",
      "train_loss:tensor(0.4929, device='cuda:0')\n",
      "val_loss:tensor(0.5661, device='cuda:0')\n",
      "epoch58\n",
      "train_loss:tensor(0.5242, device='cuda:0')\n",
      "val_loss:tensor(0.5572, device='cuda:0')\n",
      "epoch59\n",
      "train_loss:tensor(0.4849, device='cuda:0')\n",
      "val_loss:tensor(0.5921, device='cuda:0')\n",
      "epoch60\n",
      "train_loss:tensor(0.5058, device='cuda:0')\n",
      "val_loss:tensor(0.5755, device='cuda:0')\n",
      "epoch61\n",
      "train_loss:tensor(0.5084, device='cuda:0')\n",
      "val_loss:tensor(0.5480, device='cuda:0')\n",
      "epoch62\n",
      "train_loss:tensor(0.4902, device='cuda:0')\n",
      "val_loss:tensor(0.5397, device='cuda:0')\n",
      "epoch63\n",
      "train_loss:tensor(0.4704, device='cuda:0')\n",
      "val_loss:tensor(0.5482, device='cuda:0')\n",
      "epoch64\n",
      "train_loss:tensor(0.4503, device='cuda:0')\n",
      "val_loss:tensor(0.5388, device='cuda:0')\n",
      "epoch65\n",
      "train_loss:tensor(0.4552, device='cuda:0')\n",
      "val_loss:tensor(0.5355, device='cuda:0')\n",
      "epoch66\n",
      "train_loss:tensor(0.4534, device='cuda:0')\n",
      "val_loss:tensor(0.5656, device='cuda:0')\n",
      "epoch67\n",
      "train_loss:tensor(0.4431, device='cuda:0')\n",
      "val_loss:tensor(0.5437, device='cuda:0')\n",
      "epoch68\n",
      "train_loss:tensor(0.4490, device='cuda:0')\n",
      "val_loss:tensor(0.5305, device='cuda:0')\n",
      "epoch69\n",
      "train_loss:tensor(0.4404, device='cuda:0')\n",
      "val_loss:tensor(0.5245, device='cuda:0')\n",
      "epoch70\n",
      "train_loss:tensor(0.4447, device='cuda:0')\n",
      "val_loss:tensor(0.5168, device='cuda:0')\n",
      "saved best model!\n",
      "epoch71\n",
      "train_loss:tensor(0.4567, device='cuda:0')\n",
      "val_loss:tensor(0.5353, device='cuda:0')\n",
      "epoch72\n",
      "train_loss:tensor(0.4441, device='cuda:0')\n",
      "val_loss:tensor(0.5461, device='cuda:0')\n",
      "epoch73\n",
      "train_loss:tensor(0.4637, device='cuda:0')\n",
      "val_loss:tensor(0.5213, device='cuda:0')\n",
      "epoch74\n",
      "train_loss:tensor(0.4627, device='cuda:0')\n",
      "val_loss:tensor(0.5222, device='cuda:0')\n",
      "epoch75\n",
      "train_loss:tensor(0.4493, device='cuda:0')\n",
      "val_loss:tensor(0.5329, device='cuda:0')\n",
      "epoch76\n",
      "train_loss:tensor(0.4340, device='cuda:0')\n",
      "val_loss:tensor(0.5468, device='cuda:0')\n",
      "epoch77\n",
      "train_loss:tensor(0.4292, device='cuda:0')\n",
      "val_loss:tensor(0.5443, device='cuda:0')\n",
      "epoch78\n",
      "train_loss:tensor(0.4376, device='cuda:0')\n",
      "val_loss:tensor(0.5524, device='cuda:0')\n",
      "epoch79\n",
      "train_loss:tensor(0.4297, device='cuda:0')\n",
      "val_loss:tensor(0.5596, device='cuda:0')\n",
      "epoch80\n",
      "train_loss:tensor(0.4228, device='cuda:0')\n",
      "val_loss:tensor(0.5387, device='cuda:0')\n",
      "epoch81\n",
      "train_loss:tensor(0.4350, device='cuda:0')\n",
      "val_loss:tensor(0.5348, device='cuda:0')\n",
      "epoch82\n",
      "train_loss:tensor(0.4370, device='cuda:0')\n",
      "val_loss:tensor(0.5379, device='cuda:0')\n",
      "epoch83\n",
      "train_loss:tensor(0.4265, device='cuda:0')\n",
      "val_loss:tensor(0.5478, device='cuda:0')\n",
      "epoch84\n",
      "train_loss:tensor(0.4207, device='cuda:0')\n",
      "val_loss:tensor(0.5544, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m train_loss_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# 1エポック分の誤差を累積しておくための変数\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m#学習モードであることを明示\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     36\u001b[0m     x, t \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mChestDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     27\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path_list[index]\n\u001b[1;32m     28\u001b[0m label_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_path_list[index]\n\u001b[0;32m---> 29\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_axis(img)\n\u001b[1;32m     31\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(label_path))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/PIL/Image.py:675\u001b[0m, in \u001b[0;36mImage.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    673\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ArrayData(new), dtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/PIL/Image.py:718\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    716\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 718\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# unpack data\u001b[39;00m\n\u001b[1;32m    721\u001b[0m e \u001b[38;5;241m=\u001b[39m _getencoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, encoder_name, args)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.2/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# モデルの定義\n",
    "model = UNet()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# optimizerの準備\n",
    "optimizer = optim.RAdam(model.parameters())\n",
    "\n",
    "# 誤差関数の定義\n",
    "#criterion = BCEDiceLoss()\n",
    "bce = nn.BCELoss()\n",
    "sl = NaiveSizeLoss()\n",
    "\n",
    "# 訓練ループ\n",
    "\n",
    "batchsize = 5\n",
    "\n",
    "train_size = len(chest_train)\n",
    "val_size = len(chest_val)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "train_loss_list = [] # epoch毎のtrain_lossを保存しておくための入れ物\n",
    "val_loss_list = [] # epoch毎のvalidation_lossを保存しておくための入れ物\n",
    "\n",
    "loss_min = 100000\n",
    "\n",
    "#bounds = torch.Tensor([13175.0, 29502.0])\n",
    "bounds = torch.Tensor([25000.0, 29502.0])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_add = 0 # 1エポック分の誤差を累積しておくための変数\n",
    "    model.train() #学習モードであることを明示\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, t = data\n",
    "        x = torch.tensor(x) / 255\n",
    "        t = torch.tensor(t).float()\n",
    "        \n",
    "        x = x.to(\"cuda\")\n",
    "        t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x)\n",
    "        \n",
    "        bce_loss = bce(predict, t)\n",
    "        sl_loss = sl(predict, bounds)\n",
    "        loss = bce_loss + sl_loss\n",
    "        #loss = bce_loss\n",
    "        model.zero_grad()\n",
    "        loss.backward() # 誤差逆伝播法により，各パラメータについての勾配を求める\n",
    "        optimizer.step() # 上で求めた勾配を用いて，山が低くなっている方へ一歩進む\n",
    "        \n",
    "        train_loss_add += loss.data\n",
    "        \n",
    "    train_loss_mean = train_loss_add / int(train_size/batchsize)\n",
    "    print(\"epoch\" + str(epoch+1))\n",
    "    #print(bce_loss, sl_loss)\n",
    "    print(\"train_loss:\" + str(train_loss_mean))\n",
    "    train_loss_list.append(train_loss_mean.cpu())\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss_add = 0\n",
    "    num = 0\n",
    "    for i, data in enumerate(val_loader):\n",
    "            \n",
    "        #cudaに変換\n",
    "        x, t = data\n",
    "        x = torch.tensor(x) / 255\n",
    "        t = torch.tensor(t).float()\n",
    "        x = x.to(\"cuda\")\n",
    "        t = t.to(\"cuda\")\n",
    "        predict = model(x)\n",
    "        \n",
    "        loss = bce(predict, t)\n",
    "        val_loss_add += loss.data\n",
    "        \n",
    "    val_loss_mean = val_loss_add / int(val_size/batchsize)\n",
    "    print(\"val_loss:\" + str(val_loss_mean))\n",
    "    val_loss_list.append(val_loss_mean.cpu())\n",
    "    \n",
    "    if val_loss_mean < loss_min:\n",
    "        torch.save(model.state_dict(), \"/takaya_workspace/self_study/deep_learning/segmentation/models/best.model\")\n",
    "        print(\"saved best model!\")\n",
    "        loss_min = val_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d469ec-d03e-41e9-b3c4-0858ac083ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f106048f790>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4dc2078-f2bb-432a-9cf5-6034ad2e3ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8127/3183670863.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_8127/3183670863.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n",
      "/tmp/ipykernel_8127/3183670863.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_8127/3183670863.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DICE係数とIOUについて説明，実装\n",
    "# 質的評価のための可視化方法\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def thresholding(inference,  threshold=0.5):\n",
    "    # 入力は2チャンネルのVariable\n",
    "    inference = inference.data.cpu() # 最後の一枚の１チャンネル目\n",
    "\n",
    "    mask1 = inference >= threshold\n",
    "    inference[mask1] = 255\n",
    "\n",
    "    mask0 = inference < threshold\n",
    "    inference[mask0] = 0\n",
    "    \n",
    "    return inference\n",
    "\n",
    "def calc_all(tp, pp):\n",
    "    mask = pp != 0\n",
    "    pp[mask] = 1\n",
    "    tn, fp, fn, tp = confusion_matrix(tp.flatten(), pp.flatten()).ravel()\n",
    "    presicion = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    dice = tp / (tp + ((1/2)*(fp+fn)))\n",
    "    iou = tp / (tp + fp + fn)\n",
    "    return presicion, recall, dice, iou\n",
    "\n",
    "test_size = len(chest_test)\n",
    "\n",
    "iou_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "dice_list = []\n",
    "test_loss_add = 0\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "num = 0\n",
    "\n",
    "model = UNet(input_channel=1, output_channel=1)\n",
    "model = model.to(\"cuda\")\n",
    "model_path = '/takaya_workspace/self_study/deep_learning/segmentation/models/best.model'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "dice_list = []\n",
    "iou_list = []\n",
    "\n",
    "for i,data in enumerate(test_loader):\n",
    "    x, t = data\n",
    "    x = torch.tensor(x) / 255\n",
    "    t = torch.tensor(t).float()\n",
    "    \n",
    "    x = x.to(\"cuda\")\n",
    "    t = t.to(\"cuda\")\n",
    "        \n",
    "    predict = model(x)\n",
    "    predict_imgs = thresholding(predict, threshold=0.5)\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        img = np.array(predict_imgs[j][0])\n",
    "        xx = np.array(x[j][0].cpu())\n",
    "        tt = np.array(t[j][0].cpu())\n",
    "        \n",
    "        precision, recall, dice, iou = calc_all(img/255, tt)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        dice_list.append(dice)\n",
    "        iou_list.append(iou)\n",
    "    \n",
    "        plt.subplot(1, 3, 1)#1桁目 -- グラフの行数、2桁目 -- グラフの列数、3桁目 -- グラフの番号、subplot(2,3,1)の記載でも良い。\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"input\")\n",
    "        plt.imshow(xx, cmap = \"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"target\")\n",
    "        plt.imshow(tt, cmap = \"gray\")\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"output\\n(iou=\" + str(iou) + \")\")\n",
    "        plt.imshow(img, cmap = \"gray\")\n",
    "        plt.savefig(\"/takaya_workspace/self_study/deep_learning/segmentation/results/\" + str(i) + \"_\" + str(j) + \".png\")\n",
    "\n",
    "precision_list = np.array(precision_list)\n",
    "recall_list = np.array(recall_list)\n",
    "recall_list = np.nan_to_num(recall_list)\n",
    "dice_list = np.array(dice_list)\n",
    "iou_list = np.array(iou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f8c03f-e6ef-4706-a858-4cd9f2d9d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20862.0\n",
      "17155.0\n",
      "14825.0\n",
      "13175.0\n",
      "18577.0\n",
      "24999.0\n",
      "22108.0\n",
      "16084.0\n",
      "18535.0\n",
      "17977.0\n",
      "23153.0\n",
      "24524.0\n",
      "22361.0\n",
      "15265.0\n",
      "19724.0\n",
      "23169.0\n",
      "25627.0\n",
      "19298.0\n",
      "19090.0\n",
      "16348.0\n",
      "20433.0\n",
      "29502.0\n",
      "19148.0\n",
      "19397.0\n",
      "20325.0\n",
      "21373.0\n",
      "21558.0\n",
      "16999.0\n",
      "18802.0\n",
      "20906.0\n",
      "16925.0\n",
      "19799.0\n",
      "13711.0\n",
      "18784.0\n",
      "20239.0\n",
      "19283.0\n",
      "18421.0\n",
      "19968.0\n",
      "16340.0\n",
      "18576.0\n"
     ]
    }
   ],
   "source": [
    "label_list = glob.glob(\"/takaya_workspace/self_study/deep_learning/segmentation/data/train/label/*\")\n",
    "for i in range(len(label_list)):\n",
    "    label = Image.open(label_list[i])\n",
    "    label = np.array(label) / 255\n",
    "    print(label.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "598348ff-e826-4ce5-a29a-4f7c5133984b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d18f9ee-7b22-4c02-927b-667e1e9f1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093704eb-3cf9-42c0-9179-e9978a919642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3caa573-b7c5-4657-9991-1fc4093b92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
