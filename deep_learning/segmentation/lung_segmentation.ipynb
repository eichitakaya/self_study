{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13147ac-d640-4369-b53e-dfe23c83b63f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **肺セグメンテーションをやってみよう**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78716cb-c39c-4803-9fef-7b3273172d36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **深層学習によるセグメンテーション**\n",
    "セグメンテーションの課題に対しては，「ピクセル毎の分類問題を解く」という問題設定で深層学習を適用することが一般的です．\n",
    "最も簡単な例としては，正方形のパッチを切り取っては，中心のピクセルが関心領域であるかを分類することを，ピクセル数だけ繰り返す方法があります（図）（引用）．\n",
    "\n",
    "## **U-Netによるセグメンテーション**\n",
    "今回は，画像部会【リンク】にて公開されている胸部X線写真における，肺野を抽出する課題に挑戦しましょう．\n",
    "\n",
    "## **全体の流れ**\n",
    "- データの読み込み\n",
    "- U-Netの定義\n",
    "- 誤差関数の定義\n",
    "- 評価指標の定義\n",
    "- 学習ループの設計\n",
    "- モデルの学習\n",
    "- モデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f602833-f84d-44b2-a9e1-e3ca5e8a7412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2357ae12-59b1-481b-9d74-9a29ee92780e",
   "metadata": {},
   "source": [
    "### **データセットの読み込み**\n",
    "data\\\n",
    "&emsp; \\- train\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\\\n",
    "&emsp; \\- val\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\\\n",
    "&emsp; \\- test\\\n",
    "&emsp;&emsp;&emsp; \\- label\\\n",
    "&emsp;&emsp;&emsp; \\- org\n",
    "\n",
    "上記のフォルダ構造を念頭に置きながら，データローダを作ります．\\\n",
    "データをtrain, validation, testに分ける意義については，【リンク】をご覧ください．\\\n",
    "画像部会が公開している形式では，trainとtestのみに分かれていますが，trainのうち10例をvalとして分け直しました．\\\n",
    "\\\n",
    "まずは必要なライブラリをインポートしましょう．\n",
    "今回使うライブラリは以下の通りです．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379f51d2-8743-49ac-bfac-c83ebc558bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305e4a2-b999-4b8b-af2e-fef35697f34d",
   "metadata": {},
   "source": [
    "# **ミニバッチ学習のおさらい**\n",
    "<img src=\"imgs/minibatch.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742b75cf-f0f7-461d-be23-dad17772d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestDataset(Dataset):\n",
    "    def __init__(self, train=True, shuffle=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if train == True:\n",
    "            self.img_path_list = sorted(glob.glob(\"data/train/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"data/train/label/*\"))\n",
    "        else:\n",
    "            self.img_path_list = sorted(glob.glob(\"data/test/org/*\"))\n",
    "            self.label_path_list = sorted(glob.glob(\"data/test/label/*\"))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.img_path_list[index]\n",
    "        label_path = self.label_path_list[index]\n",
    "        img = np.array(Image.open(image_path))\n",
    "        img = add_axis(img)\n",
    "        label = np.array(Image.open(label_path))\n",
    "        label = self._label_transform(label)\n",
    "        label = add_axis(label)\n",
    "        return img, label\n",
    "    \n",
    "    def _label_transform(self, label):# transformの関数に書き換える\n",
    "        nonzero = np.nonzero(label)\n",
    "        label[nonzero] = 1\n",
    "        return label\n",
    "    \n",
    "    def add_axis(arr):# transformの関数に書き換える\n",
    "        arr_new = np.expand_dims(arr, 0)\n",
    "        return arr_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46b90b-130e-47e1-bb95-4e71f44e60fc",
   "metadata": {},
   "source": [
    "Transformについて説明する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af2d06b-e33f-4350-bb8a-cd4b7f0b6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_axis(arr):\n",
    "    arr_new = np.expand_dims(arr, 0)\n",
    "    return arr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d63b8d20-ecd9-416e-bbf2-5fd63bb5d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_train = ChestDataset(train=True, shuffle=True)\n",
    "chest_test = ChestDataset(train=False, shuffle=False)\n",
    "train_loader = DataLoader(chest_train, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(chest_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714eed25-42f9-4339-8da9-4c87141a0bbf",
   "metadata": {},
   "source": [
    "train_loaderがきちんと画像を読み込めているか確認しましょう．\\\n",
    "ここでshapeについて詳しく説明する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b87fb66-f515-4737-9f27-7137e89417f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n",
      "torch.Size([5, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c63366-3694-4386-8e7a-f376fa346cdb",
   "metadata": {},
   "source": [
    "### **ネットワークの定義**\n",
    "<img src=\"imgs/unet.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d572f467-2d3d-4297-802f-3b3b4dbae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=2, input_channel=1, output_channel=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        \n",
    "        self.enco1_1 = nn.Conv2d(self.input_channel, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.enco2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.enco5_1 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.enco5_2 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco6_1 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco6_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco7_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco7_2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco8_1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco8_2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.deco9_1 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.deco9_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(64, self.output_channel, kernel_size=1)\n",
    "\n",
    "        self.bn1_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn1_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn2_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn2_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn3_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn3_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn4_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn4_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn5_1 = nn.BatchNorm2d(  1024)\n",
    "        self.bn5_2 = nn.BatchNorm2d(  512)\n",
    "\n",
    "        self.bn6_1 = nn.BatchNorm2d(  512)\n",
    "        self.bn6_2 = nn.BatchNorm2d(  256)\n",
    "\n",
    "        self.bn7_1 = nn.BatchNorm2d(  256)\n",
    "        self.bn7_2 = nn.BatchNorm2d(  128)\n",
    "\n",
    "        self.bn8_1 = nn.BatchNorm2d(  128)\n",
    "        self.bn8_2 = nn.BatchNorm2d(  64)\n",
    "\n",
    "        self.bn9_1 = nn.BatchNorm2d(  64)\n",
    "        self.bn9_2 = nn.BatchNorm2d(  64)\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  \n",
    "\n",
    "    def forward(self, x): #x = (batchsize, 3, 360, 480)\n",
    "        #if LRN:\n",
    "        #    x = F.local_response_normalization(x) #Needed for preventing from overfitting\n",
    "\n",
    "        h1_1 = F.relu(self.bn1_1(self.enco1_1(x)))\n",
    "        h1_2 = F.relu(self.bn1_2(self.enco1_2(h1_1)))\n",
    "        pool1, pool1_indice = F.max_pool2d(h1_2, 2, stride=2, return_indices=True) #(batchsize,  64, 180, 240)\n",
    "\n",
    "        h2_1 = F.relu(self.bn2_1(self.enco2_1(pool1)))\n",
    "        h2_2 = F.relu(self.bn2_2(self.enco2_2(h2_1)))\n",
    "        pool2, pool2_indice = F.max_pool2d(h2_2, 2, stride=2, return_indices=True) #(batchsize, 128,  90, 120) \n",
    "\n",
    "        h3_1 = F.relu(self.bn3_1(self.enco3_1(pool2)))\n",
    "        h3_2 = F.relu(self.bn3_2(self.enco3_2(h3_1)))\n",
    "        pool3, pool3_indice = F.max_pool2d(h3_2, 2, stride=2, return_indices=True) #(batchsize, 256,  45,  60) \n",
    "\n",
    "        h4_1 = F.relu(self.bn4_1(self.enco4_1(pool3)))\n",
    "        h4_2 = F.relu(self.bn4_2(self.enco4_2(h4_1)))\n",
    "        pool4, pool4_indice = F.max_pool2d(h4_2, 2, stride=2, return_indices=True) #(batchsize, 256,  23,  30) \n",
    "\n",
    "        h5_1 = F.relu(self.bn5_1(self.enco5_1(pool4)))\n",
    "        h5_2 = F.relu(self.bn5_2(self.enco5_2(h5_1)))\n",
    "        \n",
    "        up5 = F.max_unpool2d(h5_2, pool4_indice, kernel_size=2, stride=2, output_size=(pool3.shape[2], pool3.shape[3]))\n",
    "        h6_1 = F.relu(self.bn6_1(self.deco6_1(torch.cat((up5, h4_2), dim=1))))\n",
    "        h6_2 = F.relu(self.bn6_2(self.deco6_2(h6_1)))\n",
    "\n",
    "        up6 = F.max_unpool2d(h6_2, pool3_indice, kernel_size=2, stride=2, output_size=(pool2.shape[2], pool2.shape[3]))\n",
    "        h7_1 = F.relu(self.bn7_1(self.deco7_1(torch.cat((up6, h3_2), dim=1))))\n",
    "        h7_2 = F.relu(self.bn7_2(self.deco7_2(h7_1)))\n",
    "\n",
    "        up7 = F.max_unpool2d(h7_2, pool2_indice, kernel_size=2, stride=2, output_size=(pool1.shape[2], pool1.shape[3]))\n",
    "        h8_1 = F.relu(self.bn8_1(self.deco8_1(torch.cat((up7, h2_2), dim=1))))\n",
    "        h8_2 = F.relu(self.bn8_2(self.deco8_2(h8_1)))\n",
    "\n",
    "        up8 = F.max_unpool2d(h8_2, pool1_indice, kernel_size=2, stride=2, output_size=(x.shape[2], x.shape[3])) #x = (batchsize, 128, 360, 480)\n",
    "        h9_1 = F.relu(self.bn9_1(self.deco9_1(torch.cat((up8, h1_2), dim=1))))\n",
    "        h9_2 = F.relu(self.bn9_2(self.deco9_2(h9_1)))\n",
    "\n",
    "        h = self.final_layer(h9_2)\n",
    "        #print(h.shape)\n",
    "        #print(t.shape)\n",
    "        predict = h\n",
    "        #loss = \tnn.BCEWithLogitsLoss(h, t)\n",
    "        \n",
    "        #predict = nn.Softmax(h)\n",
    "        return torch.sigmoid(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2e45fee-cf3e-44fd-bcda-014736e064c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target)\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    if input.is_cuda:\n",
    "        s = torch.FloatTensor(1).cuda().zero_()\n",
    "    else:\n",
    "        s = torch.FloatTensor(1).zero_()\n",
    "\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "\n",
    "    return s / (i + 1)\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
    "        smooth = 1e-5\n",
    "        input = torch.sigmoid(input)\n",
    "        num = target.size(0)\n",
    "        input = input.view(num, -1)\n",
    "        target = target.view(num, -1)\n",
    "        intersection = (input * target)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "        return 0.5 * bce + dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e2c78-6ad8-40c7-9546-59117fa8d12d",
   "metadata": {},
   "source": [
    "### 学習ループの作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34c9dd2a-f2bd-41a5-9cdf-1adb2b2454c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36234/3969496758.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_36234/3969496758.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "train_loss:tensor(3.0998, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36234/3969496758.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x) / 255\n",
      "/tmp/ipykernel_36234/3969496758.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2\n",
      "train_loss:tensor(2.3974, device='cuda:0')\n",
      "epoch3\n",
      "train_loss:tensor(1.7527, device='cuda:0')\n",
      "epoch4\n",
      "train_loss:tensor(1.2409, device='cuda:0')\n",
      "epoch5\n",
      "train_loss:tensor(1.0304, device='cuda:0')\n",
      "epoch6\n",
      "train_loss:tensor(0.8997, device='cuda:0')\n",
      "epoch7\n",
      "train_loss:tensor(0.8478, device='cuda:0')\n",
      "epoch8\n",
      "train_loss:tensor(0.7624, device='cuda:0')\n",
      "epoch9\n",
      "train_loss:tensor(0.7214, device='cuda:0')\n",
      "epoch10\n",
      "train_loss:tensor(0.7198, device='cuda:0')\n",
      "epoch11\n",
      "train_loss:tensor(0.6549, device='cuda:0')\n",
      "epoch12\n",
      "train_loss:tensor(0.6096, device='cuda:0')\n",
      "epoch13\n",
      "train_loss:tensor(0.5567, device='cuda:0')\n",
      "epoch14\n",
      "train_loss:tensor(0.5771, device='cuda:0')\n",
      "epoch15\n",
      "train_loss:tensor(0.5592, device='cuda:0')\n",
      "epoch16\n",
      "train_loss:tensor(0.5214, device='cuda:0')\n",
      "epoch17\n",
      "train_loss:tensor(0.4976, device='cuda:0')\n",
      "epoch18\n",
      "train_loss:tensor(0.4560, device='cuda:0')\n",
      "epoch19\n",
      "train_loss:tensor(0.4319, device='cuda:0')\n",
      "epoch20\n",
      "train_loss:tensor(0.4142, device='cuda:0')\n",
      "epoch21\n",
      "train_loss:tensor(0.3856, device='cuda:0')\n",
      "epoch22\n",
      "train_loss:tensor(0.3672, device='cuda:0')\n",
      "epoch23\n",
      "train_loss:tensor(0.3502, device='cuda:0')\n",
      "epoch24\n",
      "train_loss:tensor(0.3523, device='cuda:0')\n",
      "epoch25\n",
      "train_loss:tensor(0.3254, device='cuda:0')\n",
      "epoch26\n",
      "train_loss:tensor(0.3016, device='cuda:0')\n",
      "epoch27\n",
      "train_loss:tensor(0.2855, device='cuda:0')\n",
      "epoch28\n",
      "train_loss:tensor(0.2808, device='cuda:0')\n",
      "epoch29\n",
      "train_loss:tensor(0.2806, device='cuda:0')\n",
      "epoch30\n",
      "train_loss:tensor(0.2949, device='cuda:0')\n",
      "epoch31\n",
      "train_loss:tensor(0.2859, device='cuda:0')\n",
      "epoch32\n",
      "train_loss:tensor(0.2862, device='cuda:0')\n",
      "epoch33\n",
      "train_loss:tensor(0.2863, device='cuda:0')\n",
      "epoch34\n",
      "train_loss:tensor(0.2781, device='cuda:0')\n",
      "epoch35\n",
      "train_loss:tensor(0.2687, device='cuda:0')\n",
      "epoch36\n",
      "train_loss:tensor(0.2481, device='cuda:0')\n",
      "epoch37\n",
      "train_loss:tensor(0.2495, device='cuda:0')\n",
      "epoch38\n",
      "train_loss:tensor(0.2570, device='cuda:0')\n",
      "epoch39\n",
      "train_loss:tensor(0.2543, device='cuda:0')\n",
      "epoch40\n",
      "train_loss:tensor(0.2464, device='cuda:0')\n",
      "epoch41\n",
      "train_loss:tensor(0.2273, device='cuda:0')\n",
      "epoch42\n",
      "train_loss:tensor(0.2164, device='cuda:0')\n",
      "epoch43\n",
      "train_loss:tensor(0.2019, device='cuda:0')\n",
      "epoch44\n",
      "train_loss:tensor(0.1889, device='cuda:0')\n",
      "epoch45\n",
      "train_loss:tensor(0.1779, device='cuda:0')\n",
      "epoch46\n",
      "train_loss:tensor(0.1687, device='cuda:0')\n",
      "epoch47\n",
      "train_loss:tensor(0.1644, device='cuda:0')\n",
      "epoch48\n",
      "train_loss:tensor(0.1560, device='cuda:0')\n",
      "epoch49\n",
      "train_loss:tensor(0.1539, device='cuda:0')\n",
      "epoch50\n",
      "train_loss:tensor(0.1687, device='cuda:0')\n",
      "epoch51\n",
      "train_loss:tensor(0.1802, device='cuda:0')\n",
      "epoch52\n",
      "train_loss:tensor(0.3643, device='cuda:0')\n",
      "epoch53\n",
      "train_loss:tensor(0.4152, device='cuda:0')\n",
      "epoch54\n",
      "train_loss:tensor(0.3164, device='cuda:0')\n",
      "epoch55\n",
      "train_loss:tensor(0.2834, device='cuda:0')\n",
      "epoch56\n",
      "train_loss:tensor(0.2445, device='cuda:0')\n",
      "epoch57\n",
      "train_loss:tensor(0.2270, device='cuda:0')\n",
      "epoch58\n",
      "train_loss:tensor(0.2498, device='cuda:0')\n",
      "epoch59\n",
      "train_loss:tensor(0.2306, device='cuda:0')\n",
      "epoch60\n",
      "train_loss:tensor(0.2076, device='cuda:0')\n",
      "epoch61\n",
      "train_loss:tensor(0.1838, device='cuda:0')\n",
      "epoch62\n",
      "train_loss:tensor(0.1750, device='cuda:0')\n",
      "epoch63\n",
      "train_loss:tensor(0.1705, device='cuda:0')\n",
      "epoch64\n",
      "train_loss:tensor(0.1644, device='cuda:0')\n",
      "epoch65\n",
      "train_loss:tensor(0.1597, device='cuda:0')\n",
      "epoch66\n",
      "train_loss:tensor(0.1617, device='cuda:0')\n",
      "epoch67\n",
      "train_loss:tensor(0.1468, device='cuda:0')\n",
      "epoch68\n",
      "train_loss:tensor(0.1469, device='cuda:0')\n",
      "epoch69\n",
      "train_loss:tensor(0.1489, device='cuda:0')\n",
      "epoch70\n",
      "train_loss:tensor(0.1416, device='cuda:0')\n",
      "epoch71\n",
      "train_loss:tensor(0.1368, device='cuda:0')\n",
      "epoch72\n",
      "train_loss:tensor(0.1271, device='cuda:0')\n",
      "epoch73\n",
      "train_loss:tensor(0.1222, device='cuda:0')\n",
      "epoch74\n",
      "train_loss:tensor(0.1181, device='cuda:0')\n",
      "epoch75\n",
      "train_loss:tensor(0.1184, device='cuda:0')\n",
      "epoch76\n",
      "train_loss:tensor(0.1156, device='cuda:0')\n",
      "epoch77\n",
      "train_loss:tensor(0.1117, device='cuda:0')\n",
      "epoch78\n",
      "train_loss:tensor(0.1090, device='cuda:0')\n",
      "epoch79\n",
      "train_loss:tensor(0.1022, device='cuda:0')\n",
      "epoch80\n",
      "train_loss:tensor(0.1015, device='cuda:0')\n",
      "epoch81\n",
      "train_loss:tensor(0.1048, device='cuda:0')\n",
      "epoch82\n",
      "train_loss:tensor(0.0982, device='cuda:0')\n",
      "epoch83\n",
      "train_loss:tensor(0.0963, device='cuda:0')\n",
      "epoch84\n",
      "train_loss:tensor(0.0894, device='cuda:0')\n",
      "epoch85\n",
      "train_loss:tensor(0.0910, device='cuda:0')\n",
      "epoch86\n",
      "train_loss:tensor(0.0891, device='cuda:0')\n",
      "epoch87\n",
      "train_loss:tensor(0.0842, device='cuda:0')\n",
      "epoch88\n",
      "train_loss:tensor(0.0896, device='cuda:0')\n",
      "epoch89\n",
      "train_loss:tensor(0.0898, device='cuda:0')\n",
      "epoch90\n",
      "train_loss:tensor(0.0853, device='cuda:0')\n",
      "epoch91\n",
      "train_loss:tensor(0.0825, device='cuda:0')\n",
      "epoch92\n",
      "train_loss:tensor(0.0825, device='cuda:0')\n",
      "epoch93\n",
      "train_loss:tensor(0.0805, device='cuda:0')\n",
      "epoch94\n",
      "train_loss:tensor(0.0810, device='cuda:0')\n",
      "epoch95\n",
      "train_loss:tensor(0.0812, device='cuda:0')\n",
      "epoch96\n",
      "train_loss:tensor(0.0841, device='cuda:0')\n",
      "epoch97\n",
      "train_loss:tensor(0.0787, device='cuda:0')\n",
      "epoch98\n",
      "train_loss:tensor(0.0751, device='cuda:0')\n",
      "epoch99\n",
      "train_loss:tensor(0.0729, device='cuda:0')\n",
      "epoch100\n",
      "train_loss:tensor(0.0693, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# モデルの定義\n",
    "model = UNet()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# optimizerの準備\n",
    "optimizer = optim.RAdam(model.parameters())\n",
    "\n",
    "# 誤差関数の定義\n",
    "#criterion = BCEDiceLoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 訓練ループ\n",
    "\n",
    "batchsize = 5\n",
    "\n",
    "train_size = len(train_loader)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_loss_list = [] # epoch毎のtrain_lossを保存しておくための入れ物\n",
    "val_loss_list = [] # epoch毎のvalidation_lossを保存しておくための入れ物\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_add = 0 # 1エポック分の誤差を累積しておくための変数\n",
    "    model.train() #学習モードであることを明示\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x, t = data\n",
    "        x = torch.tensor(x) / 255\n",
    "        t = torch.tensor(t).float()\n",
    "        \n",
    "        x = x.to(\"cuda\")\n",
    "        t = t.to(\"cuda\")\n",
    "        \n",
    "        predict = model(x)\n",
    "        \n",
    "        loss = criterion(predict, t)\n",
    "        model.zero_grad()\n",
    "        loss.backward() # 誤差逆伝播法により，各パラメータについての勾配を求める\n",
    "        optimizer.step() # 上で求めた勾配を用いて，山が低くなっている方へ一歩進む\n",
    "        \n",
    "        train_loss_add += loss.data\n",
    "        \n",
    "    loss_mean = train_loss_add / int(train_size/batchsize)\n",
    "    print(\"epoch\" + str(epoch+1))\n",
    "    print(\"train_loss:\" + str(loss_mean))\n",
    "    train_loss_list.append(loss_mean)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss_add = 0\n",
    "    num = 0\n",
    "    for i, data in enumerate(test_loader):\n",
    "            \n",
    "        #cudaに変換\n",
    "        x, t = data\n",
    "        x = torch.tensor(x) / 255\n",
    "        t = torch.tensor(t).float()\n",
    "        x = x.to(\"cuda\")\n",
    "        t = t.to(\"cuda\")\n",
    "        predict = model(x)\n",
    "        \n",
    "        loss = criterion(predict, t)\n",
    "        val_loss_add += loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfae62d-d5b3-4a18-be5f-01d1c87e0ff7",
   "metadata": {},
   "source": [
    "### セグメンテーションの精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1775f61-5506-4f4c-8ea8-9ce15b413bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  1 02:18:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   49C    P0   138W / 300W |  21425MiB / 32505MiB |     77%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    67W / 300W |  10351MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    58W / 300W |  10351MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    61W / 300W |  10331MiB / 32508MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# DICE係数とIOUについて説明，実装\n",
    "# 質的評価のための可視化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d48de1-a20f-4e73-a281-38f4a7f5feb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"data/train/org/1.png\")\n",
    "img = np.array(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bec2c870-4d3c-436b-8527-771815f1cd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/train/org/22.png',\n",
       " 'data/train/org/23.png',\n",
       " 'data/train/org/4.png',\n",
       " 'data/train/org/47.png',\n",
       " 'data/train/org/9.png',\n",
       " 'data/train/org/5.png',\n",
       " 'data/train/org/7.png',\n",
       " 'data/train/org/25.png',\n",
       " 'data/train/org/1.png',\n",
       " 'data/train/org/8.png',\n",
       " 'data/train/org/12.png',\n",
       " 'data/train/org/26.png',\n",
       " 'data/train/org/39.png',\n",
       " 'data/train/org/35.png',\n",
       " 'data/train/org/28.png',\n",
       " 'data/train/org/43.png',\n",
       " 'data/train/org/21.png',\n",
       " 'data/train/org/42.png',\n",
       " 'data/train/org/34.png',\n",
       " 'data/train/org/27.png',\n",
       " 'data/train/org/32.png',\n",
       " 'data/train/org/41.png',\n",
       " 'data/train/org/13.png',\n",
       " 'data/train/org/50.png',\n",
       " 'data/train/org/36.png',\n",
       " 'data/train/org/40.png',\n",
       " 'data/train/org/49.png',\n",
       " 'data/train/org/17.png',\n",
       " 'data/train/org/3.png',\n",
       " 'data/train/org/33.png',\n",
       " 'data/train/org/18.png',\n",
       " 'data/train/org/6.png',\n",
       " 'data/train/org/48.png',\n",
       " 'data/train/org/46.png',\n",
       " 'data/train/org/31.png',\n",
       " 'data/train/org/37.png',\n",
       " 'data/train/org/11.png',\n",
       " 'data/train/org/20.png',\n",
       " 'data/train/org/29.png',\n",
       " 'data/train/org/15.png',\n",
       " 'data/train/org/2.png',\n",
       " 'data/train/org/16.png',\n",
       " 'data/train/org/30.png',\n",
       " 'data/train/org/10.png',\n",
       " 'data/train/org/14.png',\n",
       " 'data/train/org/44.png',\n",
       " 'data/train/org/19.png',\n",
       " 'data/train/org/24.png',\n",
       " 'data/train/org/45.png',\n",
       " 'data/train/org/38.png']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"data/train/org/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a43c85c-f011-45de-b2e0-c0ef9578e4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,2,0,0,2])\n",
    "mask = np.nonzero(a)\n",
    "a[mask] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c2922-9519-458d-a0cb-970e7ea3b7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
