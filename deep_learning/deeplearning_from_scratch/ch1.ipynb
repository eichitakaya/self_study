{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_spiral():\n",
    "    data = np.loadtxt(\"dataset/spiral.txt\")\n",
    "    x = data[:, 0:2]\n",
    "    t_raw = data[:, 2:3]\n",
    "    t = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        t[i][int(t_raw[i][0] - 1)] = 1\n",
    "    x = np.float32(x)\n",
    "    t = np.int8(t)\n",
    "    t_raw = np.int8(t_raw) -1\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 重みの初期化\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers  = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # すべての重みと勾配を配列にまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x,  W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "    \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  1 |  iter 10  /  10  |  loss  4.70\n",
      "| epoch  2 |  iter 10  /  10  |  loss  5.07\n",
      "| epoch  3 |  iter 10  /  10  |  loss  5.07\n",
      "| epoch  4 |  iter 10  /  10  |  loss  4.93\n",
      "| epoch  5 |  iter 10  /  10  |  loss  5.10\n",
      "| epoch  6 |  iter 10  /  10  |  loss  5.47\n",
      "| epoch  7 |  iter 10  /  10  |  loss  5.56\n",
      "| epoch  8 |  iter 10  /  10  |  loss  6.09\n",
      "| epoch  9 |  iter 10  /  10  |  loss  5.05\n",
      "| epoch  10 |  iter 10  /  10  |  loss  6.29\n",
      "| epoch  11 |  iter 10  /  10  |  loss  6.19\n",
      "| epoch  12 |  iter 10  /  10  |  loss  5.60\n",
      "| epoch  13 |  iter 10  /  10  |  loss  5.73\n",
      "| epoch  14 |  iter 10  /  10  |  loss  6.01\n",
      "| epoch  15 |  iter 10  /  10  |  loss  5.80\n",
      "| epoch  16 |  iter 10  /  10  |  loss  5.39\n",
      "| epoch  17 |  iter 10  /  10  |  loss  5.89\n",
      "| epoch  18 |  iter 10  /  10  |  loss  5.43\n",
      "| epoch  19 |  iter 10  /  10  |  loss  5.81\n",
      "| epoch  20 |  iter 10  /  10  |  loss  5.95\n",
      "| epoch  21 |  iter 10  /  10  |  loss  5.01\n",
      "| epoch  22 |  iter 10  /  10  |  loss  5.33\n",
      "| epoch  23 |  iter 10  /  10  |  loss  5.68\n",
      "| epoch  24 |  iter 10  /  10  |  loss  5.60\n",
      "| epoch  25 |  iter 10  /  10  |  loss  7.04\n",
      "| epoch  26 |  iter 10  /  10  |  loss  5.70\n",
      "| epoch  27 |  iter 10  /  10  |  loss  5.20\n",
      "| epoch  28 |  iter 10  /  10  |  loss  5.34\n",
      "| epoch  29 |  iter 10  /  10  |  loss  5.14\n",
      "| epoch  30 |  iter 10  /  10  |  loss  6.05\n",
      "| epoch  31 |  iter 10  /  10  |  loss  5.80\n",
      "| epoch  32 |  iter 10  /  10  |  loss  4.99\n",
      "| epoch  33 |  iter 10  /  10  |  loss  5.63\n",
      "| epoch  34 |  iter 10  /  10  |  loss  5.47\n",
      "| epoch  35 |  iter 10  /  10  |  loss  5.79\n",
      "| epoch  36 |  iter 10  /  10  |  loss  5.99\n",
      "| epoch  37 |  iter 10  /  10  |  loss  5.93\n",
      "| epoch  38 |  iter 10  /  10  |  loss  5.33\n",
      "| epoch  39 |  iter 10  /  10  |  loss  5.84\n",
      "| epoch  40 |  iter 10  /  10  |  loss  5.28\n",
      "| epoch  41 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  42 |  iter 10  /  10  |  loss  6.07\n",
      "| epoch  43 |  iter 10  /  10  |  loss  5.17\n",
      "| epoch  44 |  iter 10  /  10  |  loss  4.97\n",
      "| epoch  45 |  iter 10  /  10  |  loss  4.94\n",
      "| epoch  46 |  iter 10  /  10  |  loss  5.34\n",
      "| epoch  47 |  iter 10  /  10  |  loss  5.81\n",
      "| epoch  48 |  iter 10  /  10  |  loss  5.32\n",
      "| epoch  49 |  iter 10  /  10  |  loss  4.86\n",
      "| epoch  50 |  iter 10  /  10  |  loss  4.88\n",
      "| epoch  51 |  iter 10  /  10  |  loss  5.64\n",
      "| epoch  52 |  iter 10  /  10  |  loss  6.21\n",
      "| epoch  53 |  iter 10  /  10  |  loss  5.29\n",
      "| epoch  54 |  iter 10  /  10  |  loss  4.99\n",
      "| epoch  55 |  iter 10  /  10  |  loss  5.15\n",
      "| epoch  56 |  iter 10  /  10  |  loss  4.90\n",
      "| epoch  57 |  iter 10  /  10  |  loss  5.75\n",
      "| epoch  58 |  iter 10  /  10  |  loss  4.81\n",
      "| epoch  59 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  60 |  iter 10  /  10  |  loss  5.37\n",
      "| epoch  61 |  iter 10  /  10  |  loss  6.04\n",
      "| epoch  62 |  iter 10  /  10  |  loss  5.24\n",
      "| epoch  63 |  iter 10  /  10  |  loss  5.25\n",
      "| epoch  64 |  iter 10  /  10  |  loss  5.19\n",
      "| epoch  65 |  iter 10  /  10  |  loss  5.34\n",
      "| epoch  66 |  iter 10  /  10  |  loss  5.69\n",
      "| epoch  67 |  iter 10  /  10  |  loss  6.28\n",
      "| epoch  68 |  iter 10  /  10  |  loss  5.65\n",
      "| epoch  69 |  iter 10  /  10  |  loss  6.64\n",
      "| epoch  70 |  iter 10  /  10  |  loss  5.49\n",
      "| epoch  71 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  72 |  iter 10  /  10  |  loss  5.06\n",
      "| epoch  73 |  iter 10  /  10  |  loss  5.82\n",
      "| epoch  74 |  iter 10  /  10  |  loss  5.42\n",
      "| epoch  75 |  iter 10  /  10  |  loss  6.00\n",
      "| epoch  76 |  iter 10  /  10  |  loss  5.95\n",
      "| epoch  77 |  iter 10  /  10  |  loss  6.31\n",
      "| epoch  78 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  79 |  iter 10  /  10  |  loss  6.04\n",
      "| epoch  80 |  iter 10  /  10  |  loss  5.35\n",
      "| epoch  81 |  iter 10  /  10  |  loss  5.82\n",
      "| epoch  82 |  iter 10  /  10  |  loss  6.31\n",
      "| epoch  83 |  iter 10  /  10  |  loss  5.38\n",
      "| epoch  84 |  iter 10  /  10  |  loss  5.53\n",
      "| epoch  85 |  iter 10  /  10  |  loss  5.63\n",
      "| epoch  86 |  iter 10  /  10  |  loss  5.54\n",
      "| epoch  87 |  iter 10  /  10  |  loss  4.95\n",
      "| epoch  88 |  iter 10  /  10  |  loss  5.04\n",
      "| epoch  89 |  iter 10  /  10  |  loss  4.97\n",
      "| epoch  90 |  iter 10  /  10  |  loss  5.84\n",
      "| epoch  91 |  iter 10  /  10  |  loss  5.47\n",
      "| epoch  92 |  iter 10  /  10  |  loss  5.60\n",
      "| epoch  93 |  iter 10  /  10  |  loss  5.28\n",
      "| epoch  94 |  iter 10  /  10  |  loss  5.04\n",
      "| epoch  95 |  iter 10  /  10  |  loss  5.30\n",
      "| epoch  96 |  iter 10  /  10  |  loss  5.36\n",
      "| epoch  97 |  iter 10  /  10  |  loss  5.73\n",
      "| epoch  98 |  iter 10  /  10  |  loss  6.07\n",
      "| epoch  99 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  100 |  iter 10  /  10  |  loss  6.66\n",
      "| epoch  101 |  iter 10  /  10  |  loss  5.02\n",
      "| epoch  102 |  iter 10  /  10  |  loss  5.61\n",
      "| epoch  103 |  iter 10  /  10  |  loss  6.10\n",
      "| epoch  104 |  iter 10  /  10  |  loss  5.00\n",
      "| epoch  105 |  iter 10  /  10  |  loss  5.16\n",
      "| epoch  106 |  iter 10  /  10  |  loss  5.25\n",
      "| epoch  107 |  iter 10  /  10  |  loss  5.04\n",
      "| epoch  108 |  iter 10  /  10  |  loss  5.90\n",
      "| epoch  109 |  iter 10  /  10  |  loss  5.27\n",
      "| epoch  110 |  iter 10  /  10  |  loss  4.94\n",
      "| epoch  111 |  iter 10  /  10  |  loss  5.05\n",
      "| epoch  112 |  iter 10  /  10  |  loss  5.53\n",
      "| epoch  113 |  iter 10  /  10  |  loss  6.63\n",
      "| epoch  114 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  115 |  iter 10  /  10  |  loss  6.76\n",
      "| epoch  116 |  iter 10  /  10  |  loss  5.55\n",
      "| epoch  117 |  iter 10  /  10  |  loss  5.39\n",
      "| epoch  118 |  iter 10  /  10  |  loss  5.16\n",
      "| epoch  119 |  iter 10  /  10  |  loss  5.06\n",
      "| epoch  120 |  iter 10  /  10  |  loss  6.63\n",
      "| epoch  121 |  iter 10  /  10  |  loss  5.21\n",
      "| epoch  122 |  iter 10  /  10  |  loss  5.11\n",
      "| epoch  123 |  iter 10  /  10  |  loss  5.39\n",
      "| epoch  124 |  iter 10  /  10  |  loss  5.32\n",
      "| epoch  125 |  iter 10  /  10  |  loss  5.49\n",
      "| epoch  126 |  iter 10  /  10  |  loss  5.69\n",
      "| epoch  127 |  iter 10  /  10  |  loss  5.52\n",
      "| epoch  128 |  iter 10  /  10  |  loss  5.30\n",
      "| epoch  129 |  iter 10  /  10  |  loss  5.28\n",
      "| epoch  130 |  iter 10  /  10  |  loss  5.60\n",
      "| epoch  131 |  iter 10  /  10  |  loss  6.27\n",
      "| epoch  132 |  iter 10  /  10  |  loss  5.59\n",
      "| epoch  133 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  134 |  iter 10  /  10  |  loss  5.89\n",
      "| epoch  135 |  iter 10  /  10  |  loss  5.55\n",
      "| epoch  136 |  iter 10  /  10  |  loss  6.17\n",
      "| epoch  137 |  iter 10  /  10  |  loss  6.13\n",
      "| epoch  138 |  iter 10  /  10  |  loss  6.00\n",
      "| epoch  139 |  iter 10  /  10  |  loss  6.18\n",
      "| epoch  140 |  iter 10  /  10  |  loss  6.16\n",
      "| epoch  141 |  iter 10  /  10  |  loss  6.09\n",
      "| epoch  142 |  iter 10  /  10  |  loss  6.16\n",
      "| epoch  143 |  iter 10  /  10  |  loss  6.92\n",
      "| epoch  144 |  iter 10  /  10  |  loss  6.78\n",
      "| epoch  145 |  iter 10  /  10  |  loss  5.42\n",
      "| epoch  146 |  iter 10  /  10  |  loss  5.90\n",
      "| epoch  147 |  iter 10  /  10  |  loss  5.50\n",
      "| epoch  148 |  iter 10  /  10  |  loss  5.85\n",
      "| epoch  149 |  iter 10  /  10  |  loss  5.59\n",
      "| epoch  150 |  iter 10  /  10  |  loss  5.34\n",
      "| epoch  151 |  iter 10  /  10  |  loss  5.67\n",
      "| epoch  152 |  iter 10  /  10  |  loss  5.26\n",
      "| epoch  153 |  iter 10  /  10  |  loss  5.20\n",
      "| epoch  154 |  iter 10  /  10  |  loss  4.88\n",
      "| epoch  155 |  iter 10  /  10  |  loss  5.42\n",
      "| epoch  156 |  iter 10  /  10  |  loss  5.28\n",
      "| epoch  157 |  iter 10  /  10  |  loss  5.05\n",
      "| epoch  158 |  iter 10  /  10  |  loss  6.26\n",
      "| epoch  159 |  iter 10  /  10  |  loss  6.47\n",
      "| epoch  160 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  161 |  iter 10  /  10  |  loss  5.34\n",
      "| epoch  162 |  iter 10  /  10  |  loss  5.11\n",
      "| epoch  163 |  iter 10  /  10  |  loss  4.85\n",
      "| epoch  164 |  iter 10  /  10  |  loss  5.14\n",
      "| epoch  165 |  iter 10  /  10  |  loss  5.62\n",
      "| epoch  166 |  iter 10  /  10  |  loss  5.01\n",
      "| epoch  167 |  iter 10  /  10  |  loss  5.63\n",
      "| epoch  168 |  iter 10  /  10  |  loss  6.08\n",
      "| epoch  169 |  iter 10  /  10  |  loss  5.74\n",
      "| epoch  170 |  iter 10  /  10  |  loss  6.24\n",
      "| epoch  171 |  iter 10  /  10  |  loss  5.59\n",
      "| epoch  172 |  iter 10  /  10  |  loss  6.63\n",
      "| epoch  173 |  iter 10  /  10  |  loss  7.21\n",
      "| epoch  174 |  iter 10  /  10  |  loss  7.74\n",
      "| epoch  175 |  iter 10  /  10  |  loss  7.36\n",
      "| epoch  176 |  iter 10  /  10  |  loss  6.90\n",
      "| epoch  177 |  iter 10  /  10  |  loss  6.83\n",
      "| epoch  178 |  iter 10  /  10  |  loss  7.09\n",
      "| epoch  179 |  iter 10  /  10  |  loss  6.36\n",
      "| epoch  180 |  iter 10  /  10  |  loss  6.59\n",
      "| epoch  181 |  iter 10  /  10  |  loss  5.71\n",
      "| epoch  182 |  iter 10  /  10  |  loss  5.85\n",
      "| epoch  183 |  iter 10  /  10  |  loss  5.75\n",
      "| epoch  184 |  iter 10  /  10  |  loss  6.49\n",
      "| epoch  185 |  iter 10  /  10  |  loss  5.85\n",
      "| epoch  186 |  iter 10  /  10  |  loss  5.77\n",
      "| epoch  187 |  iter 10  /  10  |  loss  6.32\n",
      "| epoch  188 |  iter 10  /  10  |  loss  5.37\n",
      "| epoch  189 |  iter 10  /  10  |  loss  5.44\n",
      "| epoch  190 |  iter 10  /  10  |  loss  6.76\n",
      "| epoch  191 |  iter 10  /  10  |  loss  5.85\n",
      "| epoch  192 |  iter 10  /  10  |  loss  5.26\n",
      "| epoch  193 |  iter 10  /  10  |  loss  5.38\n",
      "| epoch  194 |  iter 10  /  10  |  loss  5.88\n",
      "| epoch  195 |  iter 10  /  10  |  loss  5.86\n",
      "| epoch  196 |  iter 10  /  10  |  loss  5.74\n",
      "| epoch  197 |  iter 10  /  10  |  loss  5.29\n",
      "| epoch  198 |  iter 10  /  10  |  loss  5.84\n",
      "| epoch  199 |  iter 10  /  10  |  loss  5.98\n",
      "| epoch  200 |  iter 10  /  10  |  loss  6.25\n",
      "| epoch  201 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  202 |  iter 10  /  10  |  loss  5.52\n",
      "| epoch  203 |  iter 10  /  10  |  loss  5.73\n",
      "| epoch  204 |  iter 10  /  10  |  loss  5.21\n",
      "| epoch  205 |  iter 10  /  10  |  loss  5.90\n",
      "| epoch  206 |  iter 10  /  10  |  loss  6.18\n",
      "| epoch  207 |  iter 10  /  10  |  loss  6.44\n",
      "| epoch  208 |  iter 10  /  10  |  loss  5.99\n",
      "| epoch  209 |  iter 10  /  10  |  loss  6.40\n",
      "| epoch  210 |  iter 10  /  10  |  loss  5.74\n",
      "| epoch  211 |  iter 10  /  10  |  loss  5.99\n",
      "| epoch  212 |  iter 10  /  10  |  loss  5.93\n",
      "| epoch  213 |  iter 10  /  10  |  loss  6.11\n",
      "| epoch  214 |  iter 10  /  10  |  loss  5.76\n",
      "| epoch  215 |  iter 10  /  10  |  loss  5.31\n",
      "| epoch  216 |  iter 10  /  10  |  loss  5.40\n",
      "| epoch  217 |  iter 10  /  10  |  loss  5.68\n",
      "| epoch  218 |  iter 10  /  10  |  loss  5.57\n",
      "| epoch  219 |  iter 10  /  10  |  loss  5.52\n",
      "| epoch  220 |  iter 10  /  10  |  loss  5.47\n",
      "| epoch  221 |  iter 10  /  10  |  loss  4.95\n",
      "| epoch  222 |  iter 10  /  10  |  loss  5.23\n",
      "| epoch  223 |  iter 10  /  10  |  loss  4.97\n",
      "| epoch  224 |  iter 10  /  10  |  loss  5.75\n",
      "| epoch  225 |  iter 10  /  10  |  loss  5.03\n",
      "| epoch  226 |  iter 10  /  10  |  loss  5.38\n",
      "| epoch  227 |  iter 10  /  10  |  loss  5.57\n",
      "| epoch  228 |  iter 10  /  10  |  loss  5.86\n",
      "| epoch  229 |  iter 10  /  10  |  loss  5.25\n",
      "| epoch  230 |  iter 10  /  10  |  loss  5.22\n",
      "| epoch  231 |  iter 10  /  10  |  loss  5.23\n",
      "| epoch  232 |  iter 10  /  10  |  loss  4.90\n",
      "| epoch  233 |  iter 10  /  10  |  loss  4.96\n",
      "| epoch  234 |  iter 10  /  10  |  loss  5.39\n",
      "| epoch  235 |  iter 10  /  10  |  loss  5.19\n",
      "| epoch  236 |  iter 10  /  10  |  loss  5.38\n",
      "| epoch  237 |  iter 10  /  10  |  loss  5.56\n",
      "| epoch  238 |  iter 10  /  10  |  loss  5.06\n",
      "| epoch  239 |  iter 10  /  10  |  loss  6.16\n",
      "| epoch  240 |  iter 10  /  10  |  loss  6.43\n",
      "| epoch  241 |  iter 10  /  10  |  loss  5.75\n",
      "| epoch  242 |  iter 10  /  10  |  loss  5.66\n",
      "| epoch  243 |  iter 10  /  10  |  loss  5.55\n",
      "| epoch  244 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  245 |  iter 10  /  10  |  loss  5.11\n",
      "| epoch  246 |  iter 10  /  10  |  loss  5.15\n",
      "| epoch  247 |  iter 10  /  10  |  loss  5.33\n",
      "| epoch  248 |  iter 10  /  10  |  loss  5.58\n",
      "| epoch  249 |  iter 10  /  10  |  loss  6.37\n",
      "| epoch  250 |  iter 10  /  10  |  loss  5.42\n",
      "| epoch  251 |  iter 10  /  10  |  loss  7.25\n",
      "| epoch  252 |  iter 10  /  10  |  loss  5.28\n",
      "| epoch  253 |  iter 10  /  10  |  loss  5.58\n",
      "| epoch  254 |  iter 10  /  10  |  loss  5.78\n",
      "| epoch  255 |  iter 10  /  10  |  loss  6.15\n",
      "| epoch  256 |  iter 10  /  10  |  loss  6.76\n",
      "| epoch  257 |  iter 10  /  10  |  loss  5.82\n",
      "| epoch  258 |  iter 10  /  10  |  loss  5.65\n",
      "| epoch  259 |  iter 10  /  10  |  loss  6.22\n",
      "| epoch  260 |  iter 10  /  10  |  loss  5.47\n",
      "| epoch  261 |  iter 10  /  10  |  loss  5.90\n",
      "| epoch  262 |  iter 10  /  10  |  loss  5.67\n",
      "| epoch  263 |  iter 10  /  10  |  loss  6.27\n",
      "| epoch  264 |  iter 10  /  10  |  loss  6.02\n",
      "| epoch  265 |  iter 10  /  10  |  loss  6.88\n",
      "| epoch  266 |  iter 10  /  10  |  loss  5.97\n",
      "| epoch  267 |  iter 10  /  10  |  loss  5.87\n",
      "| epoch  268 |  iter 10  /  10  |  loss  5.72\n",
      "| epoch  269 |  iter 10  /  10  |  loss  5.77\n",
      "| epoch  270 |  iter 10  /  10  |  loss  6.33\n",
      "| epoch  271 |  iter 10  /  10  |  loss  6.23\n",
      "| epoch  272 |  iter 10  /  10  |  loss  5.72\n",
      "| epoch  273 |  iter 10  /  10  |  loss  5.83\n",
      "| epoch  274 |  iter 10  /  10  |  loss  6.50\n",
      "| epoch  275 |  iter 10  /  10  |  loss  6.05\n",
      "| epoch  276 |  iter 10  /  10  |  loss  6.30\n",
      "| epoch  277 |  iter 10  /  10  |  loss  6.26\n",
      "| epoch  278 |  iter 10  /  10  |  loss  6.38\n",
      "| epoch  279 |  iter 10  /  10  |  loss  6.14\n",
      "| epoch  280 |  iter 10  /  10  |  loss  6.24\n",
      "| epoch  281 |  iter 10  /  10  |  loss  6.52\n",
      "| epoch  282 |  iter 10  /  10  |  loss  5.85\n",
      "| epoch  283 |  iter 10  /  10  |  loss  6.06\n",
      "| epoch  284 |  iter 10  /  10  |  loss  5.38\n",
      "| epoch  285 |  iter 10  /  10  |  loss  5.59\n",
      "| epoch  286 |  iter 10  /  10  |  loss  5.00\n",
      "| epoch  287 |  iter 10  /  10  |  loss  4.87\n",
      "| epoch  288 |  iter 10  /  10  |  loss  5.13\n",
      "| epoch  289 |  iter 10  /  10  |  loss  5.15\n",
      "| epoch  290 |  iter 10  /  10  |  loss  5.67\n",
      "| epoch  291 |  iter 10  /  10  |  loss  5.39\n",
      "| epoch  292 |  iter 10  /  10  |  loss  5.83\n",
      "| epoch  293 |  iter 10  /  10  |  loss  5.46\n",
      "| epoch  294 |  iter 10  /  10  |  loss  6.13\n",
      "| epoch  295 |  iter 10  /  10  |  loss  6.13\n",
      "| epoch  296 |  iter 10  /  10  |  loss  5.79\n",
      "| epoch  297 |  iter 10  /  10  |  loss  5.50\n",
      "| epoch  298 |  iter 10  /  10  |  loss  5.57\n",
      "| epoch  299 |  iter 10  /  10  |  loss  5.72\n",
      "| epoch  300 |  iter 10  /  10  |  loss  5.72\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "x, t = load_spiral()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "loss_list = []\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1) * batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1) * batch_size]\n",
    "        \n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch % d |  iter %d  /  %d  |  loss  %.2f' % (epoch+1, iters+1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.loadtxt(\"dataset/spiral.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d38c5b956323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_spiral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-8c5f9f03b677>\u001b[0m in \u001b[0;36mload_spiral\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "x = a[:, 0:2].shape\n",
    "t = a[:, 2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.zeros_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"dataset/spiral.txt\")\n",
    "x = data[:, 0:2]\n",
    "t_raw = data[:, 2:3]\n",
    "t = np.zeros_like(data)\n",
    "for i in range(len(data)):\n",
    "    t[i][int(t_raw[i][0] - 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (30,) (30,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-71583024d4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (30,) (30,3) "
     ]
    }
   ],
   "source": [
    "batch_t[np.arange(30), batch_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ -1.96931422e-06,  -1.35015159e-06,  -1.96988530e-06,\n",
       "          -1.13426720e-06,  -1.25350033e-06,  -1.32029090e-06,\n",
       "          -1.06664948e-06,  -1.30208235e-06,  -7.12881055e-07,\n",
       "          -1.58535959e-06],\n",
       "        [ -4.61829950e-07,  -3.58590605e-07,  -4.62239400e-07,\n",
       "          -3.22711200e-07,  -3.42616525e-07,  -3.53676630e-07,\n",
       "          -3.12241142e-07,  -3.50547161e-07,  -2.55689591e-07,\n",
       "          -3.97336299e-07]]),\n",
       " array([ -1.12881612e-07,  -7.92910983e-08,  -1.12925891e-07,\n",
       "         -6.75937109e-08,  -7.40574609e-08,  -7.76749570e-08,\n",
       "         -6.39657723e-08,  -7.66835928e-08,  -4.49187931e-08,\n",
       "         -9.20264679e-08]),\n",
       " array([[-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766],\n",
       "        [-0.29960085, -0.35046816, -0.31659766]]),\n",
       " array([-0.29960085, -0.35046816, -0.31659766])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
